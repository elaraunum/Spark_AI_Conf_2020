{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# Best Practices"],"metadata":{}},{"cell_type":"markdown","source":["[**Scaling Deep Learning Best Practices**](https://databricks-prod-cloudfront.cloud.databricks.com/public/793177bc53e528530b06c78a4fa0e086/0/14560762/100107/latest.html)\n* Use a GPU\n* Early Stopping\n* Larger batch size + learning rate\n* Use Petastorm\n* Use Multiple GPUs with Horovod"],"metadata":{}},{"cell_type":"markdown","source":["[**ULMFiT - Language Model Fine-tuning**](https://arxiv.org/pdf/1801.06146.pdf)\n\n* Discriminative Fine Tuning: Different LR per layer\n* Slanted triangular learning rates: Linearly increase learning rate, followed by linear decrease in learning rate\n* Gradual Unfreezing: Unfreeze last layer and train for one epoch and keep unfreezing layers until all layers trained/terminal layer"],"metadata":{}},{"cell_type":"markdown","source":["[**Bag of Tricks for CNN**](https://arxiv.org/pdf/1812.01187.pdf)\n* Use Xavier Initalization\n* Learning rate warmup (start with low LR and change to a higher LR)\n* Increase learning rate for larger batch size\n* No regularization on bias/no weight decay for bias terms\n* Knowledge Distillation: Use a more complex model to train a smaller model by adjusting the loss to include difference in softmax values between the more accurate and smaller model \n* Label Smoothing: Adjust labels so that softmax output will have probability 1 - ε for the correct class and ε/(K − 1) for the incorrect class, K is the number of labels\n* Image Augmentation:\n  * Random crops of rectangular areas in image\n  * Random flips \n  * Adjust hue, saturation, brightness\n  * Add PCA noise with a coefficient sampled from a normal distribution"],"metadata":{}},{"cell_type":"markdown","source":["[**fast.ai best practices**](https://forums.fast.ai/t/30-best-practices/12344)\n* Do as much of your work as you can on a small sample of the data\n* Batch normalization works best when done after ReLU\n* Data Augmentation: Use the right kind of augmentation (e.g. don't flip a cat upside down, but satellite image OK)"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"DL 11 - Best Practices","notebookId":1391719663531076},"nbformat":4,"nbformat_minor":0}
