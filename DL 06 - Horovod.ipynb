{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# Horovod\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n - Use Horovod to train a distributed neural network\n  \nHorovodRunner is a general API to run distributed DL workloads on Databricks using Uber’s [Horovod](https://github.com/uber/horovod) framework. By integrating Horovod with Spark’s barrier mode, Databricks is able to provide higher stability for long-running deep learning training jobs on Spark. HorovodRunner takes a Python method that contains DL training code with Horovod hooks. This method gets pickled on the driver and sent to Spark workers. A Horovod MPI job is embedded as a Spark job using barrier execution mode. The first executor collects the IP addresses of all task executors using BarrierTaskContext and triggers a Horovod job using mpirun. Each Python MPI process loads the pickled program back, deserializes it, and runs it.\n\n<br>\n\n![](https://files.training.databricks.com/images/horovod-runner.png)\n\nFor additional resources, see:\n* [Horovod Runner Docs](https://docs.microsoft.com/en-us/azure/databricks/applications/deep-learning/distributed-training/horovod-runner)\n* [Horovod Runner webinar](https://vimeo.com/316872704/e79235f62c)"],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## Build Model"],"metadata":{}},{"cell_type":"code","source":["import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\ntf.random.set_seed(42)\n\ndef build_model():\n  return Sequential([Dense(20, input_dim=8, activation=\"relu\"),\n                     Dense(20, activation=\"relu\"),\n                     Dense(1, activation=\"linear\")]) # Keep the output layer as linear because this is a regression problem"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["## Shard Data\n\nFrom the [Horovod docs](https://github.com/horovod/horovod/blob/master/docs/concepts.rst):\n\nHorovod core principles are based on the MPI concepts size, rank, local rank, allreduce, allgather, and broadcast. These are best explained by example. Say we launched a training script on 4 servers, each having 4 GPUs. If we launched one copy of the script per GPU:\n\n* Size would be the number of processes, in this case, 16.\n\n* Rank would be the unique process ID from 0 to 15 (size - 1).\n\n* Local rank would be the unique process ID within the server from 0 to 3.\n\nWe need to shard our data across our processes.  **NOTE:** We are using a Pandas DataFrame for demo purposes. In the next notebook we will use Parquet files with Petastorm for better scalability."],"metadata":{}},{"cell_type":"code","source":["from sklearn.datasets.california_housing import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\ndef get_dataset(rank=0, size=1):\n  scaler = StandardScaler()\n  cal_housing = fetch_california_housing(data_home=f\"{working_dir}/{rank}/\")\n  X_train, X_test, y_train, y_test = train_test_split(cal_housing.data,\n                                                      cal_housing.target,\n                                                      test_size=0.2,\n                                                      random_state=1)\n  scaler.fit(X_train)\n  X_train = scaler.transform(X_train[rank::size])\n  y_train = y_train[rank::size]\n  X_test = scaler.transform(X_test[rank::size])\n  y_test = y_test[rank::size]\n  return (X_train, y_train), (X_test, y_test)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["## Horovod"],"metadata":{}},{"cell_type":"code","source":["import horovod.tensorflow.keras as hvd\nfrom tensorflow.keras import optimizers\nimport tensorflow.keras.backend as K\n\ndef run_training_horovod():\n  # Horovod: initialize Horovod\n  hvd.init()\n    # If using GPU: pin GPU to be used to process local rank (one GPU per process)\n#   gpus = tf.config.experimental.list_physical_devices('GPU')\n#   for gpu in gpus:\n#       tf.config.experimental.set_memory_growth(gpu, True)\n#   if gpus:\n#       tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\n  \n  print(f\"Rank is: {hvd.rank()}\")\n  print(f\"Size is: {hvd.size()}\")\n  \n  (X_train, y_train), (X_test, y_test) = get_dataset(hvd.rank(), hvd.size())\n  \n  model = build_model()\n  \n  # Horovod: adjust learning rate based on number of GPUs/CPUs\n  optimizer = optimizers.Adam(lr=0.001*hvd.size())\n  \n  # Horovod: add Horovod Distributed Optimizer\n  optimizer = hvd.DistributedOptimizer(optimizer)\n  \n  # In Tensorflow 2.x specify `experimental_run_tf_function=False` to ensure TensorFlow uses hvd.DistributedOptimizer() to compute gradients.\n  # See horovod docs at https://horovod.readthedocs.io/en/latest/tensorflow.html\n  model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mse\"], experimental_run_tf_function=False)\n  \n  history = model.fit(X_train, y_train, validation_split=.2, epochs=10, batch_size=64, verbose=2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["Test it out on just the driver."],"metadata":{}},{"cell_type":"code","source":["from sparkdl import HorovodRunner\n\nhr = HorovodRunner(np=-1)\nhr.run(run_training_horovod)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">HorovodRunner will stream all training logs to notebook cell output. If there are too many logs, you\ncan adjust the log level in your train method. Or you can set driver_log_verbosity to\n&#39;log_callback_only&#39; and use a HorovodRunner log  callback on the first worker to get concise\nprogress updates.\nHorovodRunner will launch Horovod jobs on the driver node. There would be resource contention if you\nshare the cluster with others.\nThe global names read or written to by the pickled function are {&#39;hvd&#39;, &#39;build_model&#39;, &#39;print&#39;, &#39;get_dataset&#39;, &#39;optimizers&#39;}.\nThe pickled object size is 2966 bytes.\n\n### How to enable Horovod Timeline? ###\nHorovodRunner has the ability to record the timeline of its activity with Horovod  Timeline. To\nrecord a Horovod Timeline, set the `HOROVOD_TIMELINE` environment variable  to the location of the\ntimeline file to be created. You can then open the timeline file  using the chrome://tracing\nfacility of the Chrome browser.\n\nExecuting command: [&#39;mpirun&#39;, &#39;--allow-run-as-root&#39;, &#39;-np&#39;, &#39;1&#39;, &#39;-H&#39;, &#39;localhost&#39;, &#39;--stdin&#39;, &#39;none&#39;, &#39;--tag-output&#39;, &#39;-mca&#39;, &#39;rmaps&#39;, &#39;seq&#39;, &#39;--bind-to&#39;, &#39;none&#39;, &#39;-x&#39;, &#39;NCCL_DEBUG=INFO&#39;, &#39;-mca&#39;, &#39;pml&#39;, &#39;ob1&#39;, &#39;-mca&#39;, &#39;btl&#39;, &#39;^openib&#39;, &#39;-mca&#39;, &#39;plm_rsh_agent&#39;, &#39;ssh -o StrictHostKeyChecking=no -i /tmp/HorovodRunner_c72c1a05b4e7/id_rsa&#39;, &#39;bash&#39;, &#39;/tmp/HorovodRunner_c72c1a05b4e7/launch.sh&#39;].\n\n[1,0]&lt;stdout&gt;:Rank is: 0\n[1,0]&lt;stdout&gt;:Size is: 1\n[1,0]&lt;stderr&gt;:Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /dbfs/user/odl_user_195841@databrickslabs.onmicrosoft.com/deep_learning/dl_06_horovod/0/\n[1,0]&lt;stderr&gt;:Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /dbfs/user/odl_user_195841@databrickslabs.onmicrosoft.com/deep_learning/dl_06_horovod/0/\n[1,0]&lt;stderr&gt;:2020-06-22 19:16:35.743381: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n[1,0]&lt;stderr&gt;:2020-06-22 19:16:35.750577: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2294685000 Hz\n[1,0]&lt;stderr&gt;:2020-06-22 19:16:35.750967: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55eabdda06e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n[1,0]&lt;stderr&gt;:2020-06-22 19:16:35.750999: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n[1,0]&lt;stdout&gt;:Epoch 1/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 2.6824 - mse: 2.6824 - val_loss: 0.8186 - val_mse: 0.8186\n[1,0]&lt;stdout&gt;:Epoch 2/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.7604 - mse: 0.7604 - val_loss: 0.6136 - val_mse: 0.6136\n[1,0]&lt;stdout&gt;:Epoch 3/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.5593 - mse: 0.5593 - val_loss: 0.4940 - val_mse: 0.4940\n[1,0]&lt;stdout&gt;:Epoch 4/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.4559 - mse: 0.4559 - val_loss: 0.4383 - val_mse: 0.4383\n[1,0]&lt;stdout&gt;:Epoch 5/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.4141 - mse: 0.4141 - val_loss: 0.4185 - val_mse: 0.4185\n[1,0]&lt;stdout&gt;:Epoch 6/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.3951 - mse: 0.3951 - val_loss: 0.4142 - val_mse: 0.4142\n[1,0]&lt;stdout&gt;:Epoch 7/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.3837 - mse: 0.3837 - val_loss: 0.3952 - val_mse: 0.3952\n[1,0]&lt;stdout&gt;:Epoch 8/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.3737 - mse: 0.3737 - val_loss: 0.3898 - val_mse: 0.3898\n[1,0]&lt;stdout&gt;:Epoch 9/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.3670 - mse: 0.3670 - val_loss: 0.3832 - val_mse: 0.3832\n[1,0]&lt;stdout&gt;:Epoch 10/10\n[1,0]&lt;stdout&gt;:207/207 - 0s - loss: 0.3623 - mse: 0.3623 - val_loss: 0.3761 - val_mse: 0.3761\n</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["# Better Horovod"],"metadata":{}},{"cell_type":"code","source":["from tensorflow.keras import optimizers\nfrom tensorflow.keras.callbacks import *\n\ndef run_training_horovod():\n  # Horovod: initialize Horovod.\n  hvd.init()\n  \n#  # If using GPU: pin GPU to be used to process local rank (one GPU per process)\n#   gpus = tf.config.experimental.list_physical_devices('GPU')\n#   for gpu in gpus:\n#       tf.config.experimental.set_memory_growth(gpu, True)\n#   if gpus:\n#       tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\n\n\n  print(f\"Rank is: {hvd.rank()}\")\n  print(f\"Size is: {hvd.size()}\")\n  \n  (X_train, y_train), (X_test, y_test) = get_dataset(hvd.rank(), hvd.size())\n  \n  model = build_model()\n  \n  # Horovod: adjust learning rate based on number of GPUs.\n  optimizer = optimizers.Adam(lr=0.001*hvd.size())\n  \n  # Horovod: add Horovod Distributed Optimizer.\n  optimizer = hvd.DistributedOptimizer(optimizer)\n\n  # In Tensorflow 2.x specify `experimental_run_tf_function=False` to ensure TensorFlow uses hvd.DistributedOptimizer() to compute gradients.\n  # See horovod docs at https://horovod.readthedocs.io/en/latest/keras.html\n  model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mse\"], experimental_run_tf_function=False)\n\n  # Use the optimized FUSE Mount\n  checkpoint_dir = f\"{working_dir}/horovod_checkpoint_weights.ckpt\"\n  \n  callbacks = [\n    # Horovod: broadcast initial variable states from rank 0 to all other processes.\n    # This is necessary to ensure consistent initialization of all workers when\n    # training is started with random weights or restored from a checkpoint.\n    hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n\n    # Horovod: average metrics among workers at the end of every epoch.\n    # Note: This callback must be in the list before the ReduceLROnPlateau,\n    # TensorBoard or other metrics-based callbacks.\n    hvd.callbacks.MetricAverageCallback(),\n\n    # Horovod: using `lr = 1.0 * hvd.size()` from the very beginning leads to worse final\n    # accuracy. Scale the learning rate `lr = 1.0` ---> `lr = 1.0 * hvd.size()` during\n    # the first five epochs. See https://arxiv.org/abs/1706.02677 for details.\n    hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=5, verbose=1),\n    \n    # Reduce the learning rate if training plateaus.\n    ReduceLROnPlateau(patience=10, verbose=1)\n  ]\n  \n  # Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\n  if hvd.rank() == 0:\n    callbacks.append(ModelCheckpoint(checkpoint_dir, save_weights_only=True))\n  \n  history = model.fit(X_train, y_train, validation_split=.2, epochs=10, batch_size=64, verbose=2, callbacks=callbacks)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["Test it out on just the driver."],"metadata":{}},{"cell_type":"code","source":["hr = HorovodRunner(np=-1)\nhr.run(run_training_horovod)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">HorovodRunner will stream all training logs to notebook cell output. If there are too many logs, you\ncan adjust the log level in your train method. Or you can set driver_log_verbosity to\n&#39;log_callback_only&#39; and use a HorovodRunner log  callback on the first worker to get concise\nprogress updates.\nHorovodRunner will launch Horovod jobs on the driver node. There would be resource contention if you\nshare the cluster with others.\nThe global names read or written to by the pickled function are {&#39;ModelCheckpoint&#39;, &#39;hvd&#39;, &#39;build_model&#39;, &#39;ReduceLROnPlateau&#39;, &#39;print&#39;, &#39;working_dir&#39;, &#39;get_dataset&#39;, &#39;optimizers&#39;}.\nThe pickled object size is 3435 bytes.\n\n### How to enable Horovod Timeline? ###\nHorovodRunner has the ability to record the timeline of its activity with Horovod  Timeline. To\nrecord a Horovod Timeline, set the `HOROVOD_TIMELINE` environment variable  to the location of the\ntimeline file to be created. You can then open the timeline file  using the chrome://tracing\nfacility of the Chrome browser.\n\nExecuting command: [&#39;mpirun&#39;, &#39;--allow-run-as-root&#39;, &#39;-np&#39;, &#39;1&#39;, &#39;-H&#39;, &#39;localhost&#39;, &#39;--stdin&#39;, &#39;none&#39;, &#39;--tag-output&#39;, &#39;-mca&#39;, &#39;rmaps&#39;, &#39;seq&#39;, &#39;--bind-to&#39;, &#39;none&#39;, &#39;-x&#39;, &#39;NCCL_DEBUG=INFO&#39;, &#39;-mca&#39;, &#39;pml&#39;, &#39;ob1&#39;, &#39;-mca&#39;, &#39;btl&#39;, &#39;^openib&#39;, &#39;-mca&#39;, &#39;plm_rsh_agent&#39;, &#39;ssh -o StrictHostKeyChecking=no -i /tmp/HorovodRunner_c21242bdafe8/id_rsa&#39;, &#39;bash&#39;, &#39;/tmp/HorovodRunner_c21242bdafe8/launch.sh&#39;].\n\n[1,0]&lt;stdout&gt;:Rank is: 0\n[1,0]&lt;stdout&gt;:Size is: 1\n[1,0]&lt;stderr&gt;:2020-06-22 19:19:33.431496: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n[1,0]&lt;stderr&gt;:2020-06-22 19:19:33.436231: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2294685000 Hz\n[1,0]&lt;stderr&gt;:2020-06-22 19:19:33.436555: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5586bd5a1300 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n[1,0]&lt;stderr&gt;:2020-06-22 19:19:33.436587: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n[1,0]&lt;stdout&gt;:Epoch 1/10\n[1,0]&lt;stderr&gt;:WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.148933). Check your callbacks.\n[1,0]&lt;stderr&gt;:Method (on_train_batch_end) is slow compared to the batch update (0.148933). Check your callbacks.\n[1,0]&lt;stdout&gt;:207/207 - 1s - loss: 1.7693 - mse: 1.7693 - val_loss: 0.6971 - val_mse: 0.6971 - lr: 0.0010\n[1,0]&lt;stdout&gt;:Epoch 2/10\n[1,0]&lt;stdout&gt;:207/207 - 1s - loss: 0.6130 - mse: 0.6130 - val_loss: 0.5420 - val_mse: 0.5420 - lr: 0.0010\n[1,0]&lt;stdout&gt;:Epoch 3/10\n[1,0]&lt;stdout&gt;:207/207 - 1s - loss: 0.4930 - mse: 0.4930 - val_loss: 0.4652 - val_mse: 0.4652 - lr: 0.0010\n[1,0]&lt;stdout&gt;:Epoch 4/10\n[1,0]&lt;stdout&gt;:207/207 - 1s - loss: 0.4337 - mse: 0.4337 - val_loss: 0.4265 - val_mse: 0.4265 - lr: 0.0010\n[1,0]&lt;stdout&gt;:Epoch 5/10\n[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:Epoch 5: finished gradual learning rate warmup to 0.001.\n[1,0]&lt;stdout&gt;:207/207 - 1s - loss: 0.4048 - mse: 0.4048 - val_loss: 0.4128 - val_mse: 0.4128 - lr: 0.0010\n[1,0]&lt;stdout&gt;:Epoch 6/10\n[1,0]&lt;stdout&gt;:207/207 - 1s - loss: 0.3904 - mse: 0.3904 - val_loss: 0.4015 - val_mse: 0.4015 - lr: 0.0010\n[1,0]&lt;stdout&gt;:Epoch 7/10\n[1,0]&lt;stdout&gt;:207/207 - 1s - loss: 0.3809 - mse: 0.3809 - val_loss: 0.3904 - val_mse: 0.3904 - lr: 0.0010\n[1,0]&lt;stdout&gt;:Epoch 8/10\n[1,0]&lt;stdout&gt;:207/207 - 1s - loss: 0.3752 - mse: 0.3752 - val_loss: 0.3849 - val_mse: 0.3849 - lr: 0.0010\n[1,0]&lt;stdout&gt;:Epoch 9/10\n[1,0]&lt;stdout&gt;:207/207 - 1s - loss: 0.3706 - mse: 0.3706 - val_loss: 0.3831 - val_mse: 0.3831 - lr: 0.0010\n[1,0]&lt;stdout&gt;:Epoch 10/10\n[1,0]&lt;stdout&gt;:207/207 - 1s - loss: 0.3654 - mse: 0.3654 - val_loss: 0.3822 - val_mse: 0.3822 - lr: 0.0010\n</div>"]}}],"execution_count":15},{"cell_type":"markdown","source":["## Run on all workers"],"metadata":{}},{"cell_type":"code","source":["## OPTIONAL: You can enable Horovod Timeline as follows, but can incur slow down from frequent writes, and have to export out of Databricks to upload to chrome://tracing\n# import os\n# os.environ[\"HOROVOD_TIMELINE\"] = f\"{working_dir}/_timeline.json\"\n\nhr = HorovodRunner(np=0)\nhr.run(run_training_horovod)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">HorovodRunner will stream all training logs to notebook cell output. If there are too many logs, you\ncan adjust the log level in your train method. Or you can set driver_log_verbosity to\n&#39;log_callback_only&#39; and use a HorovodRunner log  callback on the first worker to get concise\nprogress updates.\nSetting np=0 is deprecated and it will be removed in the next major Databricks Runtime release.\nChoosing np based on the total task slots at runtime is unreliable due to dynamic executor\nregistration. Please set the number of parallel processes you need explicitly.\nThe global names read or written to by the pickled function are {&#39;ModelCheckpoint&#39;, &#39;hvd&#39;, &#39;build_model&#39;, &#39;ReduceLROnPlateau&#39;, &#39;print&#39;, &#39;working_dir&#39;, &#39;get_dataset&#39;, &#39;optimizers&#39;}.\nThe pickled object size is 3437 bytes.\n\n### How to enable Horovod Timeline? ###\nHorovodRunner has the ability to record the timeline of its activity with Horovod  Timeline. To\nrecord a Horovod Timeline, set the `HOROVOD_TIMELINE` environment variable  to the location of the\ntimeline file to be created. You can then open the timeline file  using the chrome://tracing\nfacility of the Chrome browser.\n\nStart training.\n[1,0]&lt;stdout&gt;:Rank is: 0\n[1,0]&lt;stdout&gt;:Size is: 4\n[1,3]&lt;stdout&gt;:Rank is: 3\n[1,3]&lt;stdout&gt;:Size is: 4\n[1,1]&lt;stdout&gt;:Rank is: 1\n[1,1]&lt;stdout&gt;:Size is: 4\n[1,2]&lt;stdout&gt;:Rank is: 2\n[1,2]&lt;stdout&gt;:Size is: 4\n[1,3]&lt;stderr&gt;:Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /dbfs/user/odl_user_195841@databrickslabs.onmicrosoft.com/deep_learning/dl_06_horovod/3/\n[1,3]&lt;stderr&gt;:Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /dbfs/user/odl_user_195841@databrickslabs.onmicrosoft.com/deep_learning/dl_06_horovod/3/\n[1,0]&lt;stderr&gt;:2020-06-22 19:20:48.824932: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n[1,0]&lt;stderr&gt;:2020-06-22 19:20:48.844571: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2294685000 Hz\n[1,0]&lt;stderr&gt;:2020-06-22 19:20:48.844976: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5619a1640fd0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n[1,0]&lt;stderr&gt;:2020-06-22 19:20:48.845003: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n[1,1]&lt;stderr&gt;:Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /dbfs/user/odl_user_195841@databrickslabs.onmicrosoft.com/deep_learning/dl_06_horovod/1/\n[1,1]&lt;stderr&gt;:Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /dbfs/user/odl_user_195841@databrickslabs.onmicrosoft.com/deep_learning/dl_06_horovod/1/\n[1,2]&lt;stderr&gt;:Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /dbfs/user/odl_user_195841@databrickslabs.onmicrosoft.com/deep_learning/dl_06_horovod/2/\n[1,2]&lt;stderr&gt;:Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /dbfs/user/odl_user_195841@databrickslabs.onmicrosoft.com/deep_learning/dl_06_horovod/2/\n[1,0]&lt;stdout&gt;:Epoch 1/10[1,0]&lt;stdout&gt;:\n[1,3]&lt;stderr&gt;:2020-06-22 19:20:52.968484: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n[1,3]&lt;stderr&gt;:2020-06-22 19:20:52.978562: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2294685000 Hz\n[1,3]&lt;stderr&gt;:2020-06-22 19:20:52.978791: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b4ae2d8550 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n[1,3]&lt;stderr&gt;:2020-06-22 19:20:52.978821: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n[1,3]&lt;stdout&gt;:Epoch 1/10[1,3]&lt;stdout&gt;:\n[1,2]&lt;stderr&gt;:2020-06-22 19:20:53.259615: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n[1,2]&lt;stderr&gt;:2020-06-22 19:20:53.267845: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2294685000 Hz\n[1,2]&lt;stderr&gt;:2020-06-22 19:20:53.268083: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f2896aeae0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n[1,2]&lt;stderr&gt;:2020-06-22 19:20:53.268112: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n[1,2]&lt;stdout&gt;:Epoch 1/10\n[1,1]&lt;stderr&gt;:2020-06-22 19:20:56.549867: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n[1,1]&lt;stderr&gt;:2020-06-22 19:20:56.556415: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2294685000 Hz\n[1,1]&lt;stderr&gt;:2020-06-22 19:20:56.556671: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562cb9b1d860 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n[1,1]&lt;stderr&gt;:2020-06-22 19:20:56.556700: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n[1,1]&lt;stdout&gt;:Epoch 1/10\n[1,1]&lt;stderr&gt;:WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.539102). Check your callbacks.\n[1,1]&lt;stderr&gt;:Method (on_train_batch_end) is slow compared to the batch update (0.539102). Check your callbacks.\n[1,0]&lt;stderr&gt;:WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.269477). Check your callbacks.\n[1,0]&lt;stderr&gt;:Method (on_train_batch_end) is slow compared to the batch update (0.269477). Check your callbacks.\n[1,1]&lt;stderr&gt;:WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.269564). Check your callbacks.\n[1,3]&lt;stderr&gt;:WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.270555). Check your callbacks.\n[1,3]&lt;stderr&gt;:Method (on_train_batch_end) is slow compared to the batch update (0.270555). Check your callbacks.\n[1,1]&lt;stderr&gt;:Method (on_train_batch_end) is slow compared to the batch update (0.269564). Check your callbacks.\n[1,2]&lt;stderr&gt;:WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.266393). Check your callbacks.\n[1,2]&lt;stderr&gt;:Method (on_train_batch_end) is slow compared to the batch update (0.266393). Check your callbacks.\n[1,1]&lt;stdout&gt;:52/52 - 1s - loss: 3.2843 - mse: 3.2843 - val_loss: 1.9593 - val_mse: 1.9593 - lr: 0.0016\n[1,2]&lt;stdout&gt;:52/52 - 1s - loss: 3.2843 - mse: 3.2843 - val_loss: 1.9593 - val_mse: 1.9593 - lr: 0.0016\n[1,3]&lt;stdout&gt;:52/52 - 1s - loss: 3.2843 - mse: 3.2843 - val_loss: 1.9593 - val_mse: 1.9593 - lr: 0.0016\n[1,2]&lt;stdout&gt;:Epoch 2/10\n[1,1]&lt;stdout&gt;:Epoch 2/10\n[1,3]&lt;stdout&gt;:Epoch 2/10\n[1,0]&lt;stdout&gt;:52/52 - 1s - loss: 3.2843 - mse: 3.2843 - val_loss: 1.9593 - val_mse: 1.9593 - lr: 0.0016\n[1,0]&lt;stdout&gt;:Epoch 2/10[1,0]&lt;stdout&gt;:\n[1,3]&lt;stdout&gt;:52/52 - 1s - loss: 1.8887 - mse: 1.8887 - val_loss: 0.8469 - val_mse: 0.8469 - lr: 0.0022\n[1,2]&lt;stdout&gt;:52/52 - 1s - loss: 1.8887 - mse: 1.8887 - val_loss: 0.8469 - val_mse: 0.8469 - lr: 0.0022\n[1,1]&lt;stdout&gt;:52/52 - 1s - loss: 1.8887 - mse: 1.8887 - val_loss: 0.8469 - val_mse: 0.8469 - lr: 0.0022\n[1,3]&lt;stdout&gt;:Epoch 3/10\n[1,2]&lt;stdout&gt;:Epoch 3/10\n[1,1]&lt;stdout&gt;:Epoch 3/10\n[1,0]&lt;stdout&gt;:52/52 - 1s - loss: 1.8887 - mse: 1.8887 - val_loss: 0.8469 - val_mse: 0.8469 - lr: 0.0022\n[1,0]&lt;stdout&gt;:Epoch 3/10[1,0]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:52/52 - 1s - loss: 0.9814 - mse: 0.9814 - val_loss: 0.6263 - val_mse: 0.6263 - lr: 0.0028\n[1,2]&lt;stdout&gt;:52/52 - 1s - loss: 0.9814 - mse: 0.9814 - val_loss: 0.6263 - val_mse: 0.6263 - lr: 0.0028\n[1,3]&lt;stdout&gt;:52/52 - 1s - loss: 0.9814 - mse: 0.9814 - val_loss: 0.6263 - val_mse: 0.6263 - lr: 0.0028\n[1,1]&lt;stdout&gt;:Epoch 4/10\n[1,3]&lt;stdout&gt;:Epoch 4/10\n[1,2]&lt;stdout&gt;:Epoch 4/10[1,2]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:52/52 - 1s - loss: 0.9814 - mse: 0.9814 - val_loss: 0.6263 - val_mse: 0.6263 - lr: 0.0028\n[1,0]&lt;stdout&gt;:Epoch 4/10[1,0]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:52/52 - 1s - loss: 0.6512 - mse: 0.6512 - val_loss: 0.5131 - val_mse: 0.5131 - lr: 0.0034\n[1,3]&lt;stdout&gt;:52/52 - 1s - loss: 0.6512 - mse: 0.6512 - val_loss: 0.5131 - val_mse: 0.5131 - lr: 0.0034\n[1,1]&lt;stdout&gt;:Epoch 5/10\n[1,3]&lt;stdout&gt;:Epoch 5/10[1,3]&lt;stdout&gt;:\n[1,2]&lt;stdout&gt;:52/52 - 1s - loss: 0.6512 - mse: 0.6512 - val_loss: 0.5131 - val_mse: 0.5131 - lr: 0.0034\n[1,2]&lt;stdout&gt;:Epoch 5/10\n[1,0]&lt;stdout&gt;:52/52 - 1s - loss: 0.6512 - mse: 0.6512 - val_loss: 0.5131 - val_mse: 0.5131 - lr: 0.0034\n[1,0]&lt;stdout&gt;:Epoch 5/10\n[1,2]&lt;stdout&gt;:\n[1,2]&lt;stdout&gt;:Epoch 5: finished gradual learning rate warmup to 0.004.\n[1,1]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:Epoch 5: finished gradual learning rate warmup to 0.004.\n[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:Epoch 5: finished gradual learning rate warmup to 0.004.\n[1,1]&lt;stdout&gt;:52/52 - 1s - loss: 0.4848 - mse: 0.4848 - val_loss: 0.4538 - val_mse: 0.4538 - lr: 0.0040\n[1,2]&lt;stdout&gt;:52/52 - 1s - loss: 0.4848 - mse: 0.4848 - val_loss: 0.4538 - val_mse: 0.4538 - lr: 0.0040\n[1,3]&lt;stdout&gt;:\n[1,3]&lt;stdout&gt;:Epoch 5: finished gradual learning rate warmup to 0.004.\n[1,3]&lt;stdout&gt;:52/52 - 1s - loss: 0.4848 - mse: 0.4848 - val_loss: 0.4538 - val_mse: 0.4538 - lr: 0.0040\n[1,2]&lt;stdout&gt;:Epoch 6/10\n[1,1]&lt;stdout&gt;:Epoch 6/10\n[1,3]&lt;stdout&gt;:Epoch 6/10[1,3]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:52/52 - 1s - loss: 0.4848 - mse: 0.4848 - val_loss: 0.4538 - val_mse: 0.4538 - lr: 0.0040\n[1,0]&lt;stdout&gt;:Epoch 6/10[1,0]&lt;stdout&gt;:\n[1,3]&lt;stdout&gt;:52/52 - 0s - loss: 0.4196 - mse: 0.4196 - val_loss: 0.4228 - val_mse: 0.4228 - lr: 0.0040\n[1,2]&lt;stdout&gt;:52/52 - 0s - loss: 0.4196 - mse: 0.4196 - val_loss: 0.4228 - val_mse: 0.4228 - lr: 0.0040\n[1,1]&lt;stdout&gt;:52/52 - 0s - loss: 0.4196 - mse: 0.4196 - val_loss: 0.4228 - val_mse: 0.4228 - lr: 0.0040\n[1,3]&lt;stdout&gt;:Epoch 7/10\n[1,1]&lt;stdout&gt;:Epoch 7/10\n[1,2]&lt;stdout&gt;:Epoch 7/10\n[1,0]&lt;stdout&gt;:52/52 - 1s - loss: 0.4196 - mse: 0.4196 - val_loss: 0.4228 - val_mse: 0.4228 - lr: 0.0040\n[1,0]&lt;stdout&gt;:Epoch 7/10[1,0]&lt;stdout&gt;:\n[1,2]&lt;stdout&gt;:52/52 - 0s - loss: 0.3982 - mse: 0.3982 - val_loss: 0.4095 - val_mse: 0.4095 - lr: 0.0040\n[1,3]&lt;stdout&gt;:52/52 - 0s - loss: 0.3982 - mse: 0.3982 - val_loss: 0.4095 - val_mse: 0.4095 - lr: 0.0040\n[1,1]&lt;stdout&gt;:52/52 - 0s - loss: 0.3982 - mse: 0.3982 - val_loss: 0.4095 - val_mse: 0.4095 - lr: 0.0040\n[1,2]&lt;stdout&gt;:Epoch 8/10\n[1,1]&lt;stdout&gt;:Epoch 8/10[1,3]&lt;stdout&gt;:Epoch 8/10[1,1]&lt;stdout&gt;:\n[1,3]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:52/52 - 1s - loss: 0.3982 - mse: 0.3982 - val_loss: 0.4095 - val_mse: 0.4095 - lr: 0.0040\n[1,0]&lt;stdout&gt;:Epoch 8/10[1,0]&lt;stdout&gt;:\n[1,2]&lt;stdout&gt;:52/52 - 1s - loss: 0.3863 - mse: 0.3863 - val_loss: 0.3986 - val_mse: 0.3986 - lr: 0.0040\n[1,1]&lt;stdout&gt;:52/52 - 1s - loss: 0.3863 - mse: 0.3863 - val_loss: 0.3986 - val_mse: 0.3986 - lr: 0.0040\n[1,1]&lt;stdout&gt;:Epoch 9/10\n[1,2]&lt;stdout&gt;:Epoch 9/10[1,3]&lt;stdout&gt;:52/52 - 1s - loss: 0.3863 - mse: 0.3863 - val_loss: 0.3986 - val_mse: 0.3986 - lr: 0.0040\n[1,2]&lt;stdout&gt;:\n[1,3]&lt;stdout&gt;:Epoch 9/10\n[1,0]&lt;stdout&gt;:52/52 - 1s - loss: 0.3863 - mse: 0.3863 - val_loss: 0.3986 - val_mse: 0.3986 - lr: 0.0040\n[1,0]&lt;stdout&gt;:Epoch 9/10[1,0]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:52/52 - 1s - loss: 0.3779 - mse: 0.3779 - val_loss: 0.3926 - val_mse: 0.3926 - lr: 0.0040\n[1,2]&lt;stdout&gt;:52/52 - 1s - loss: 0.3779 - mse: 0.3779 - val_loss: 0.3926 - val_mse: 0.3926 - lr: 0.0040\n[1,3]&lt;stdout&gt;:52/52 - 1s - loss: 0.3779 - mse: 0.3779 - val_loss: 0.3926 - val_mse: 0.3926 - lr: 0.0040\n[1,2]&lt;stdout&gt;:Epoch 10/10\n[1,1]&lt;stdout&gt;:Epoch 10/10\n[1,3]&lt;stdout&gt;:Epoch 10/10\n[1,0]&lt;stdout&gt;:52/52 - 1s - loss: 0.3779 - mse: 0.3779 - val_loss: 0.3926 - val_mse: 0.3926 - lr: 0.0040\n[1,0]&lt;stdout&gt;:Epoch 10/10[1,0]&lt;stdout&gt;:\n[1,3]&lt;stdout&gt;:52/52 - 1s - loss: 0.3720 - mse: 0.3720 - val_loss: 0.3846 - val_mse: 0.3846 - lr: 0.0040\n[1,2]&lt;stdout&gt;:52/52 - 1s - loss: 0.3720 - mse: 0.3720 - val_loss: 0.3846 - val_mse: 0.3846 - lr: 0.0040\n[1,1]&lt;stdout&gt;:52/52 - 1s - loss: 0.3720 - mse: 0.3720 - val_loss: 0.3846 - val_mse: 0.3846 - lr: 0.0040\n[1,0]&lt;stdout&gt;:52/52 - 1s - loss: 0.3720 - mse: 0.3720 - val_loss: 0.3846 - val_mse: 0.3846 - lr: 0.0040\n</div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"DL 06 - Horovod","notebookId":1391719663531964},"nbformat":4,"nbformat_minor":0}
