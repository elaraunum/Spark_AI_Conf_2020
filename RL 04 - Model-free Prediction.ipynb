{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# Model-Free Prediction  \n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you learn:<br>\n - Monte-Carlo Learning for Prediction Task\n - Temporal-Difference Learning\n - \\\\(TD(\\lambda)\\\\)\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) References\n* [David Silver lecture](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ)\n* Sutton book - Chapter 5, 6, 7, 8"],"metadata":{}},{"cell_type":"markdown","source":["### What is Monte-Carlo (MC) method?\n<br>\n - Monte Carlo methods, or Monte Carlo experiments, are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results.\n  - Sampling\n  - Estimation\n  - Optimization\n - Easy to use and efficient\n - Used in different branches of science (physical processes, operations research etc.) and also in practice e.g. option pricing!"],"metadata":{}},{"cell_type":"code","source":["import numpy as np\n\ndef is_in_unit_circle(x, y, r):\n  \"\"\"This function tests whether or not a point is inside (or on the line) of a given circle.\"\"\"\n  \n  if pow(x,2)+pow(y,2)<= pow(r, 2):\n    return True\n  else:\n    return False\n\n\ndef MCM(a, r, episodes):\n  \"\"\"This function generates multiple episodes.\"\"\"\n  \n  inside_count = 0\n  for i in range (episodes):\n    x = np.random.uniform(low = -a, high = a, size = 1)\n    y = np.random.uniform(low = -a, high = a, size = 1)\n    if is_in_unit_circle(x, y, r):\n      inside_count += 1\n      \n  return inside_count"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["MCM (1, 1, 1000)/1000"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["### MC Reinforcement Learning ###\n<br>\n - MC methods learn directly from episodes of experience\n - MC is model-free: no knowledge of MDP transitions / rewards \n - MC learns from complete episodes: no bootstrapping\n - MC uses the simplest possible idea: value = mean return\n - Caveat: can only apply MC to episodic MDPs\n  - Terminal state(s) is required"],"metadata":{}},{"cell_type":"markdown","source":["### Monte-Carlo for Policy Evaluation###\n\n#### Questions ####\n\nGiven the fact that \\\\(v\\_{\\pi} = E\\_{\\pi} \\big[G\\_{t} \\bigm\\vert S\\_{t} = s \\big]\\\\), can use Monte-Carlo? If so, how? Discuss this with your neighbors.\n\nThere are multiple approaches to use MCM for value evaluation:\n\n0. First-Visit: Accumulate rewards (and counts) from the point forward **only the first time** state s is visited in an episode. Is there any guarantee that empirical mean converges to actual mean?  \n0. Every-Visit: calculate the empirical mean. Accumulate rewards (and counts) from the point forward **whenever** state is visited (possibly multiple times in an episode). \n0. Can we run 1 and 2 more efficiently? i.e. do not keep track of all the returns."],"metadata":{}},{"cell_type":"markdown","source":["### Temporal-Difference Learning###\n\n- MC methods are offline learning. It means you have to wait until end of episode to update the states.\n- TD methods learn directly from episode of experience (online learning)\n- TD methods are model free similar to MC methods\n- TD methods are useful for incomplete episodes (unlike MC methods)\n- TD updates a guess towards a guess"],"metadata":{}},{"cell_type":"markdown","source":["### How does TD work?###\n\n- Same goal: learn the value function \\\\(v\\_{\\pi}\\\\)\n- Instead of \\\\(V(S\\_{t}) \\longleftarrow V(S\\_{t}) + \\alpha \\Bigg(G\\_{t} - V(S\\_{t})\\Bigg)\\\\) (updating the value towards actual return)\n- Do  \\\\(V(S\\_{t}) \\longleftarrow V(S\\_{t}) + \\alpha \\Bigg(R\\_{t+1} +\\gamma V(S\\_{t+1})- V(S\\_{t})\\Bigg)\\\\) (update the values toward estimated return)\n- What we just did is called TD(0). Simplest form of \\\\(TD(\\lambda)\\\\) (more on this later)\n- \\\\(R\\_{t+1} +\\gamma V(S\\_{t+1})\\\\) is called TD target\n- \\\\(\\delta\\_{t} = R\\_{t+1} +\\gamma V(S\\_{t+1})- V(S\\_{t})\\\\) is TD error"],"metadata":{}},{"cell_type":"markdown","source":["### Questions ###\n\n0. What are the advantages of TD?\n0. What can you say about Bias/variance trade off for TD and MC?"],"metadata":{}},{"cell_type":"markdown","source":["### Batch MC and TD ###\n- MC and TD converge as we get more samples i.e. as gain more experience \n- What if you have limited experience? i.e. you only have finite experience (sample of observations)"],"metadata":{}},{"cell_type":"markdown","source":["### Questions ###\n\nConsider the following 5 episodes in which the agent goes through two states (A, B). What is V(A) and V(B)? \n\n0. A, 1, B, 2\n0. A, 2\n0. B, 1\n0. B, 1\n0. B, 1"],"metadata":{}},{"cell_type":"markdown","source":["### Putting all we have learned together in one picture ###\n\n<br><br><br><br>\n![rat, cheese, lever](https://slideplayer.com/slide/4856063/15/images/28/Dimensions+of+Reinforcement+Learning.jpg)"],"metadata":{}},{"cell_type":"markdown","source":["### Questions ###\n\n0. Bootstrapping refers to situation in which updates involve an estimate. What technique does bootstrap?\n0. Sampling refers to situation in which we sample a realization. What technique does sampling?"],"metadata":{}},{"cell_type":"markdown","source":["### n-step Return & n-step temporal-difference learning ###\n\nIt is not required to use only one step estimate. One can use any form of n-step return. For example:\n\n- 1-step return: \\\\(G\\_{t}^{1} = R\\_{t+1} + \\gamma V(S\\_{t+1})\\\\)\n- 2-step return: \\\\(G\\_{t}^{2} = R\\_{t+1} + \\gamma V(S\\_{t+1}) + \\gamma^2 V(S\\_{t+2} )\\\\)\n- n-step return: \\\\(G\\_{t}^{n} = R\\_{t+1} + \\gamma V(S\\_{t+1}) + \\gamma^2 V(S\\_{t+2} ) + ... + \\gamma^nV(S\\_{t+n})\\\\)\n- \\\\(V(S\\_{t}) \\longleftarrow V(S\\_{t}) + \\alpha \\Bigg(G\\_{t}^{n} - V(S\\_{t})\\Bigg)\\\\)"],"metadata":{}},{"cell_type":"markdown","source":["### Averaging n-Step Returns ###\n\n- Natural question is whether or not one can combine n-step return for different time steps. For example, you might want to do something like:\n$$ \\frac{1}{3} G^4 + \\frac{1}{3} G^5 + \\frac{1}{3} G^6$$\n- Doable in practice. One can put weight on different n-steps: the farther you get, the less weight on the return\n- Mathematically, this can be represented as:\n$$ G^\\lambda\\_{t} = (1-\\lambda) \\sum\\_{n=1}^{n = \\infty}\\lambda^{n-1}G\\_{t}^n$$\n$$ V(S\\_{t}) \\longleftarrow V(S\\_{t}) + \\alpha \\Bigg(G\\_{t}^{\\lambda} - V(S\\_{t})\\Bigg) $$"],"metadata":{}},{"cell_type":"markdown","source":["### Questions ###\n0. How does weight change over time? Can you plot that for \\\\(\\lambda = 0.5 \\\\) for \\\\(t \\in [0, 30]\\\\)?\n0. What is the total area under the curve?"],"metadata":{}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nfrom scipy import integrate\nimport numpy as np\n\ndef plot_weight(lambda_value = 0.5, max_time = 30):\n  \"\"\"This function create the plot of decaying weights\"\"\"\n  \n  time = np.arange(0, max_time, 1)\n\n  # using formula mentioned above\n  weight = (1-lambda_value) * pow(lambda_value, time)\n\n  # plotting the result\n  fig = plt.figure()\n  plt.plot(time , weight)\n  fig.suptitle(r'weight over time ($\\lambda$ = 0.5)', fontsize=20)\n  plt.xlabel('time', fontsize=18)\n  plt.ylabel('weight', fontsize=16)\n\n  # display the plot\n  plt.show()\n  display()\n\nplot_weight()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["def calculate_area(lambda_value = 0.5, max_time = 30):\n  \"\"\"This function calculates the area under curve for the given interval\"\"\"\n  \n  area = 0\n  for i in range(max_time):\n    area += (1-lambda_value) * pow(lambda_value, i)\n  return (area)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["### Forward View TD(\\\\(\\lambda\\\\))\n- Forward view: it means you look forward in time. So you need to look forward to update \\\\(G\\_{t}^{\\lambda}\\\\)\n- Downside? It is like MC. You need to have complete episode to do that. Future is only known when you have the complete realization of the underlying process.\n\n### Backward View TD(\\\\(\\lambda\\\\))\n- Update online, every step, from incomplete sequence. It provides a mechanism.\n- In order to do so we need to define two quantities for each state:\n - Frequency: assign credits to most frequent states (why?)\n - Recency: assign credit to most recent states (why?)\n- Eligibility traces combines both:\n - \\\\(E\\_{0}(s) = 0\\\\)\n - \\\\(E\\_{t}(s) = \\gamma\\lambda E\\_{t-1}(s) + 1(S\\_{t} = s)\\\\), where \\\\(1\\\\) is a indicator function\n- Keep an eligibility trace for every state s\n- Update V(s) for every state\n$$\\delta\\_{t} = R\\_{t+1} + \\gamma V(S\\_{t+1}) - V(S\\_{t}) $$\n$$ V(s) \\longleftarrow V(s) + \\alpha \\delta\\_{t}E\\_{t}(s) $$"],"metadata":{}},{"cell_type":"markdown","source":["### Questions ###\n0. What happens when \\\\(\\lambda = 0\\\\)? Why?\n0. What happens when \\\\(\\lambda = 1\\\\)? Why?"],"metadata":{}},{"cell_type":"markdown","source":["### Extra (out of scope) ###\nOne can show that. The sum of offline updates is identical for forward-view and backward-view TD(\\\\(\\lambda\\\\)). i.e. $$ \\sum\\_{t = 1}^{T} \\alpha \\delta\\_{t} E\\_{t}(s) = \\sum\\_{t =1}^{T}\\alpha\\Bigg(G\\_{t}^{\\lambda} - V(S\\_{t})\\Bigg) 1(S\\_{t} = s) $$"],"metadata":{}},{"cell_type":"markdown","source":["### Offline and Online Updates ###\n- Updates are accumulated within episode, but applied at the end of episode. In this case forward and backward TD are equivalent\n- Online updates are applied at each step within episode. In this case forward and backward TD are slightly different"],"metadata":{}},{"cell_type":"markdown","source":["### Questions ###\nDiscuss offline and online updates for forward and backward TD for different values of \\\\(\\lambda\\\\). In what scenarios are they equivalent?"],"metadata":{}},{"cell_type":"markdown","source":["### Further reading ###\n- Exact online \\\\(TD(\\lambda)\\\\): [ICML paper](http://proceedings.mlr.press/v32/seijen14.pdf)"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"RL 04 - Model-free Prediction","notebookId":2929930686998111},"nbformat":4,"nbformat_minor":0}
