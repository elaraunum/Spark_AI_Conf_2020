{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# Linear Regression\n\nBefore you attempt to throw a neural network at a problem, you want to establish a __baseline model__. Often, this will be a simple model, such as linear regression. Once we establish a baseline, then we can get started with Deep Learning.\n\nThe slides for the course can be found [here](https://brookewenig.github.io/DeepLearning.html).\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n - Build a linear regression model using scikit-learn and reimplement it in Keras \n - Modify # of epochs\n - Visualize loss"],"metadata":{}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Classroom-Setup\n\nFor each lesson to execute correctly, please make sure to run the **`Classroom-Setup`** cell at the start of each lesson (see the next cell)."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["Let's start by making a simple array of features, and the label we are trying to predict is y = 2*X + 1."],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nnp.set_printoptions(suppress=True)\n\nX = np.arange(-10, 11).reshape((21,1))\ny = 2*X + 1\n\nlist(zip(X, y))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n\nplt.plot(X, y, \"ro\", label=\"True y\")\n\nplt.title(\"X vs Y\")\nplt.xlabel(\"X\")\nplt.ylabel(\"y\")\nplt.legend()\n\nplt.show()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["Use sklearn to establish our baseline (for simplicity, we are using the same dataset for training and testing in this toy example, but we will change that later)."],"metadata":{}},{"cell_type":"code","source":["from sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\n\nlr.fit(X, y)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["from sklearn.metrics import mean_squared_error\n\ny_pred = lr.predict(X)\nmse = mean_squared_error(y, y_pred)\nprint(mse)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["### Visualize Predictions"],"metadata":{}},{"cell_type":"code","source":["plt.plot(X, y, \"ro\", label=\"True y\")\nplt.plot(X, y_pred, label=\"Pred y\")\n\nplt.title(\"X vs True Y and Pred Y (Linear Regression)\")\nplt.xlabel(\"X\")\nplt.ylabel(\"y\")\nplt.legend()\n\nplt.show()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["-sandbox\n## Keras\n\nNow that we have established a baseline model, let's see if we can build a fully-connected neural network that can meet or exceed our linear regression model. A fully-connected neural network is simply a set of matrix multiplications followed by some non-linear function (to be discussed later). \n\n[Keras](https://www.tensorflow.org/guide/keras) is a high-level API to build neural networks and was released by Fran√ßois Chollet in 2015. It is now the official high-level API of TensorFlow. \n\n##### Steps to build a Keras model\n<img style=\"width:20%\" src=\"https://files.training.databricks.com/images/5_cycle.jpg\" >"],"metadata":{}},{"cell_type":"markdown","source":["# 1. Define a N-Layer Network\n\nHere, we need to specify the dimensions of our input and output layers. When we say something is an N-layer neural network, we count all of the layers except the input layer. The diagram below demonstrates a 2-layer neural network. \n\nA special case of neural network with no hidden layers and no non-linearities is actually just linear regression :).\n\n![](https://files.training.databricks.com/images/Neural_network.svg)\n\nFor the next few labs, we will use the [Sequential model](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/keras/Sequential) from Keras."],"metadata":{}},{"cell_type":"code","source":["import tensorflow as tf\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\ntf.random.set_seed(42)\n\n# The Sequential model is a linear stack of layers.\nmodel = Sequential()\n\nmodel.add(Dense(units=1, input_dim=1, activation=\"linear\"))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["We can check the model definition by calling `.summary()`. Note the two parameters - any thoughts on why there are TWO?"],"metadata":{}},{"cell_type":"code","source":["model.summary()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["# 2. Compile a Network\n\nTo [compile](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#compile) the network, we need to specify the loss function and which optimizer to use. We'll talk more about optimizers and loss metrics in the next lab.\n\nFor right now, we will use `mse` (mean squared error) for our loss function, and the `adam` optimizer."],"metadata":{}},{"cell_type":"code","source":["model.compile(loss=\"mse\", optimizer=\"adam\")"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["# 3. Fit a Network\n\nLet's [fit](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#fit) our model on X and y."],"metadata":{}},{"cell_type":"code","source":["model.fit(X, y)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["And take a look at the predictions."],"metadata":{}},{"cell_type":"code","source":["keras_pred = model.predict(X)\nkeras_pred"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["def kerasPredPlot(keras_pred):\n  plt.clf()\n  plt.plot(X, y, \"ro\", label=\"True y\")\n  plt.plot(X, keras_pred, label=\"Pred y\")\n  \n  plt.title(\"X vs True Y and Pred Y (Keras)\")\n  plt.xlabel(\"X\")\n  plt.ylabel(\"y\")\n  plt.legend(numpoints=1)\n  plt.show()\n  \nkerasPredPlot(keras_pred)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["What went wrong?? Turns out there a few more hyperparameters we need to set. Let's take a look at [Keras documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#fit).\n\n`epochs` specifies how many passes you want over your entire dataset. Let's increase the number of epochs, and look at how the MSE decreases.\n\nHere we are capturing the output of model.fit() as it returns a History object, which keeps a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable)."],"metadata":{}},{"cell_type":"code","source":["history = model.fit(X, y, epochs=20) "],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["def viewModelLoss():\n  plt.clf()\n  plt.plot(history.history[\"loss\"])\n  \n  plt.title(\"Model Loss\")\n  plt.ylabel(\"Loss\")\n  plt.xlabel(\"Epoch\")\n  plt.show()\n  \nviewModelLoss()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["Let's try increasing the epochs even more."],"metadata":{}},{"cell_type":"code","source":["history = model.fit(X, y, epochs=4000)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["Let's inspect how our model decreased the loss (MSE) as the number of epochs increases."],"metadata":{}},{"cell_type":"code","source":["viewModelLoss()"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["Extract model weights. Wahoo! We were able to approximate y=2*X + 1 quite well! If we trained for some more epochs, we should be able to approximate this function exactly (at risk of overfitting of course)."],"metadata":{}},{"cell_type":"code","source":["print(model.get_weights())\npredicted_w = model.get_weights()[0][0][0]\npredicted_b = model.get_weights()[1][0]\n\nprint(f\"predicted_w: {predicted_w}\")\nprint(f\"predicted_b: {predicted_b}\")"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["# 4. Evaluate Network\n\nAs mentioned previously, we want to make sure our neural network can beat our benchmark."],"metadata":{}},{"cell_type":"code","source":["model.evaluate(X, y) # Prints loss value & metrics values for the model in test mode (both are MSE here)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["# 5. Make Predictions"],"metadata":{}},{"cell_type":"code","source":["keras_pred = model.predict(X)\nkeras_pred"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["kerasPredPlot(keras_pred)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["Alright, this was a very simple, contrived example. Let's go ahead and make this a bit more interesting in the next lab!"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"DL 01 - Linear Regression","notebookId":1391719663531923},"nbformat":4,"nbformat_minor":0}
