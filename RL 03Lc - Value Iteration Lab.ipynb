{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# Dynamic Programming: Value Iteration Lab\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lab you learn:<br>\n - Value Iteration\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) References\n* Sutton Chapter 3"],"metadata":{}},{"cell_type":"markdown","source":["### Value Iteration ###\n<br>\nIn this lab we are going to find an optimal policy by implementing **value iteration** algorithms. Refer to the demo notebook to find proper formula to implement this lab.\n\n![gridenv](https://miro.medium.com/max/1000/1*iX-Fu5YzUZ8CNEZ86BvfKA.png)"],"metadata":{}},{"cell_type":"markdown","source":["Recreate the environment for the Dynamic Programming from the first lab."],"metadata":{}},{"cell_type":"code","source":["%run \"./helper/GridWorldAdvancedEnvironment\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["environment = GridWorldAdvancedEnvironment()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["#ANSWER\nimport copy\ndef policy_evaluation(environment, values, gamma=0.9):\n    \"\"\"Evaluate a policy given the full information of the environment. This is not the full RL.\"\"\"\n    \n    old_values = copy.deepcopy(values)\n  \n    \n    # Action's probability at any given state\n    action_probability = 1\n    \n    # Probability of going to state s' from s if you take an action\n    probability = 1\n    \n    \n    for state in range(environment.ns):\n      # Make sure we cover all states\n      environment.set_state(state)\n      for action in range(4):\n      \n  \n        # Take an action\n        next_state, reward, is_done, _= environment.step(action)\n        # Use the formula to back up the values\n        temp = action_probability * (reward + gamma * probability * old_values[next_state])\n        if temp > values[state]:\n          values[state] = temp\n      \n        environment.set_state(state)\n  \n\n    return values"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["#ANSWER\ndef policy_iteration(environment, iteration=10000, gamma=0.9):\n  \"\"\"This function calculates the optimal value of each state using value iteration method.\"\"\"\n  \n  values = np.ones(environment.ns)\n  for j in range(iteration):\n    values = policy_evaluation(environment, values, gamma)\n  return values"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["optimal_values = policy_iteration(environment)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# Test your code\nvalue_expected = np.array([[21.97, 24.41, 21.97, 19.41, 17.47] ,[19.77, 21.97, 19.77, 17.80, 16.02], [17.80, 19.77, 17.80, 16.02, 14.41 ], [16.02, 17.80, 16.02, 14.41,12.97],[14.41,  16.02, 14.41, 12.97, 11.67]])\nnp.testing.assert_array_almost_equal(optimal_values.reshape(5,5), value_expected, err_msg = \"The values are incorrect\", decimal=2)\n"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"RL 03Lc - Value Iteration Lab","notebookId":2929930686998333},"nbformat":4,"nbformat_minor":0}
