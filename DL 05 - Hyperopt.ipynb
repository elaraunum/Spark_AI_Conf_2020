{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# Hyperopt\n\nThe [Hyperopt library](https://github.com/hyperopt/hyperopt) allows for parallel hyperparameter tuning using either random search or Tree of Parzen Estimators (TPE). With MLflow, we can record the hyperparameters and corresponding metrics for each hyperparameter combination. You can read more on [SparkTrials w/ Hyperopt](https://github.com/hyperopt/hyperopt/blob/master/docs/templates/scaleout/spark.md).\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:\n - Use Hyperopt to train and optimize a feed-forward neural net"],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["from sklearn.datasets.california_housing import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\n\ncal_housing = fetch_california_housing()\n\n# split 80/20 train-test\nX_train, X_test, y_train, y_test = train_test_split(cal_housing.data,\n                                                    cal_housing.target,\n                                                    test_size=0.2,\n                                                    random_state=1)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["## Data Standardization\nLet's do feature-wise standardization."],"metadata":{}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["-sandbox\n\n## Keras Model\n\nWe will define our NN in Keras and use the hyperparameters given by HyperOpt.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> We need to import `tensorflow` within the function due to a pickling issue.  <a href=\"https://docs.databricks.com/applications/deep-learning/single-node-training/tensorflow.html#tensorflow-2-known-issues\" target=\"_blank\">See known issues here.</a>"],"metadata":{}},{"cell_type":"code","source":["import tensorflow as tf\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\nimport mlflow\nimport mlflow.keras\ntf.random.set_seed(42)\n\ndef create_model(hpo):\n  model = Sequential()\n  model.add(Dense(int(hpo[\"dense_l1\"]), input_dim=8, activation=\"relu\"))\n  model.add(Dense(int(hpo[\"dense_l2\"]), activation=\"relu\"))\n  model.add(Dense(1, activation=\"linear\"))\n  return model"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["from hyperopt import fmin, hp, tpe, STATUS_OK, SparkTrials\n\ndef runNN(hpo):\n  # Need to include the TF import due to serialization issues\n  import tensorflow as tf\n  \n  model = create_model(hpo)\n\n  # Select Optimizer\n  optimizer_call = getattr(tf.keras.optimizers, hpo[\"optimizer\"])\n  optimizer = optimizer_call(learning_rate=hpo[\"learning_rate\"])\n\n  # Compile model\n  model.compile(loss=\"mse\",\n                optimizer=optimizer,\n                metrics=[\"mse\"])\n\n  history = model.fit(X_train, y_train, validation_split=.2, epochs=10, verbose=2)\n\n  # Evaluate our model\n  score = model.evaluate(X_test, y_test, verbose=0)\n  obj_metric = score[0]  \n  return {\"loss\": obj_metric, \"status\": STATUS_OK}"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["### Setup hyperparameter space and training\n\nWe need to create a search space for HyperOpt and set up SparkTrials to allow HyperOpt to run in parallel using Spark worker nodes. We can also start a MLflow run to automatically track the results of HyperOpt's tuning trials."],"metadata":{}},{"cell_type":"code","source":["space = {\n  \"dense_l1\": hp.quniform(\"dense_l1\", 10, 30, 1),\n  \"dense_l2\": hp.quniform(\"dense_l2\", 10, 30, 1),\n  \"learning_rate\": hp.loguniform(\"learning_rate\", -5, 0),\n  \"optimizer\": hp.choice(\"optimizer\", [\"Adadelta\", \"Adam\"])\n }\n\nspark_trials = SparkTrials(parallelism=4)\n\nwith mlflow.start_run():\n  best_hyperparam = fmin(fn=runNN, \n                         space=space, \n                         algo=tpe.suggest, \n                         max_evals=30, \n                         trials=spark_trials)\n\nbest_hyperparam"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Hyperopt with SparkTrials will automatically track trials in MLflow. To view the MLflow experiment associated with the notebook, click the &#39;Runs&#39; icon in the notebook context bar on the upper right. There, you can view all runs.\nTo view logs from trials, please check the Spark executor logs. To view executor logs, expand &#39;Spark Jobs&#39; above until you see the (i) icon next to the stage from the trial job. Click it and find the list of tasks; Task 0 is the first trial attempt, and subsequent Tasks are retries. Click the &#39;stderr&#39; link for a task to view trial logs.\n\r  0%|          | 0/30 [00:00&lt;?, ?trial/s, best loss=?]\r  3%|▎         | 1/30 [00:12&lt;05:51, 12.11s/trial, best loss: 0.6919485926628113]\r 10%|█         | 3/30 [00:15&lt;04:01,  8.94s/trial, best loss: 0.3245863914489746]\r 13%|█▎        | 4/30 [00:16&lt;02:50,  6.56s/trial, best loss: 0.3245863914489746]\r 17%|█▋        | 5/30 [00:23&lt;02:47,  6.70s/trial, best loss: 0.3245863914489746]\r 20%|██        | 6/30 [00:26&lt;02:14,  5.59s/trial, best loss: 0.3245863914489746]\r 23%|██▎       | 7/30 [00:27&lt;01:36,  4.22s/trial, best loss: 0.3245863914489746]\r 27%|██▋       | 8/30 [00:28&lt;01:11,  3.25s/trial, best loss: 0.3245863914489746]\r 30%|███       | 9/30 [00:36&lt;01:38,  4.68s/trial, best loss: 0.3245863914489746]\r 37%|███▋      | 11/30 [00:38&lt;01:07,  3.58s/trial, best loss: 0.3245863914489746]\r 40%|████      | 12/30 [00:40&lt;00:56,  3.11s/trial, best loss: 0.3245863914489746]\r 43%|████▎     | 13/30 [00:48&lt;01:17,  4.58s/trial, best loss: 0.3245863914489746]\r 47%|████▋     | 14/30 [00:49&lt;00:56,  3.51s/trial, best loss: 0.3245863914489746]\r 53%|█████▎    | 16/30 [00:51&lt;00:38,  2.76s/trial, best loss: 0.3245863914489746]\r 57%|█████▋    | 17/30 [00:59&lt;00:56,  4.34s/trial, best loss: 0.3245863914489746]\r 60%|██████    | 18/30 [01:01&lt;00:43,  3.64s/trial, best loss: 0.3245863914489746]\r 63%|██████▎   | 19/30 [01:02&lt;00:31,  2.85s/trial, best loss: 0.3245863914489746]\r 67%|██████▋   | 20/30 [01:04&lt;00:26,  2.60s/trial, best loss: 0.3245863914489746]\r 70%|███████   | 21/30 [01:10&lt;00:32,  3.63s/trial, best loss: 0.3245863914489746]\r 77%|███████▋  | 23/30 [01:13&lt;00:20,  2.99s/trial, best loss: 0.3245863914489746]\r 80%|████████  | 24/30 [01:16&lt;00:18,  3.03s/trial, best loss: 0.3234902620315552]\r 83%|████████▎ | 25/30 [01:22&lt;00:19,  3.92s/trial, best loss: 0.3234902620315552]\r 87%|████████▋ | 26/30 [01:25&lt;00:14,  3.65s/trial, best loss: 0.3234902620315552]\r 90%|█████████ | 27/30 [01:26&lt;00:08,  2.86s/trial, best loss: 0.3234902620315552]\r 93%|█████████▎| 28/30 [01:28&lt;00:05,  2.60s/trial, best loss: 0.3234902620315552]\r 97%|█████████▋| 29/30 [01:34&lt;00:03,  3.62s/trial, best loss: 0.3234902620315552]\r100%|██████████| 30/30 [01:35&lt;00:00,  2.86s/trial, best loss: 0.31949880719184875]\r100%|██████████| 30/30 [01:35&lt;00:00,  3.19s/trial, best loss: 0.31949880719184875]\nTotal Trials: 30: 30 succeeded, 0 failed, 0 cancelled.\nOut[10]: {&#39;dense_l1&#39;: 20.0,\n &#39;dense_l2&#39;: 27.0,\n &#39;learning_rate&#39;: 0.006992510192068646,\n &#39;optimizer&#39;: 1}</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["To view the MLflow experiment associated with the notebook, click the Runs icon in the notebook context bar on the upper right. There, you can view all runs. You can also bring up the full MLflow UI by clicking the button on the upper right that reads View Experiment UI when you hover over it.\n\nTo understand the effect of tuning a hyperparameter:\n\n0. Select the resulting runs and click Compare.\n0. In the Scatter Plot, select a hyperparameter for the X-axis and loss for the Y-axis."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"DL 05 - Hyperopt","notebookId":1391719663531909},"nbformat":4,"nbformat_minor":0}
