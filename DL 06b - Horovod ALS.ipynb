{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n<img src=\"https://files.training.databricks.com/images/movie-camera.png\" style=\"float:right; height: 200px; margin: 10px; border: 1px solid #ddd; border-radius: 15px 15px 15px 15px; padding: 10px\"/>\n\n# Movie Recommendations\n\nIn the previous labs, we didn't need to do any data preprocessing. In this lab, we will use our preprocessing steps from Spark as input to Horovod. \n\nHere, we will use 1 million movie ratings from the [MovieLens stable benchmark rating dataset](http://grouplens.org/datasets/movielens/). We will start by building a benchmark model with ALS, and then see if we can beat that benchmark with a neural network!\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n- Combine User + Item factors identified from ALS and use as input to a neural network\n- Create custom activation function (scaled sigmoid) to bound output of regression tasks\n- Train distributed neural network using Horovod"],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["moviesDF = spark.read.parquet(\"dbfs:/mnt/training/movielens/movies.parquet/\")\nratingsDF = spark.read.parquet(\"dbfs:/mnt/training/movielens/ratings.parquet/\")\n\nratingsDF.cache()\nmoviesDF.cache()\n\nratingsCount = ratingsDF.count()\nmoviesCount = moviesDF.count()\n\nprint(f\"There are {ratingsCount} ratings and {moviesCount} movies in the datasets\")"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["Let's take a quick look at some of the data in the two DataFrames."],"metadata":{}},{"cell_type":"code","source":["display(moviesDF)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["display(ratingsDF)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["What range of values do the ratings take?"],"metadata":{}},{"cell_type":"code","source":["display(ratingsDF.select(\"rating\").describe())"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Let's start by splitting our data into a training and test set."],"metadata":{}},{"cell_type":"code","source":["seed=42\n(trainingDF, testDF) = ratingsDF.randomSplit([0.8, 0.2], seed=seed)\n\nprint(f\"Training: {trainingDF.count()}, test: {testDF.count()}\")"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["## Alternating Least Squares Method (ALS)\n\nALS is a parallel algorithm for matrix factorization. Taking the (often low rank) matrix of every user's rating of every movie, ALS will try to find 2 lower dimensional matrices whose product will approximate the original matrix. One matrix represents users and their latent factors while the other contains movies and their latent factors. \n\nSince our goal is to be able to predict every user's rating of every movie, knowing these 2 lower dimensional matrices will give us what we need to reconstruct this information.\n\n![factorization](https://files.training.databricks.com/images/matrix_factorization.png)"],"metadata":{}},{"cell_type":"markdown","source":["Let's build and train our baseline ALS model."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.recommendation import ALS\n\nals = (ALS()\n       .setUserCol(\"userId\")\n       .setItemCol(\"movieId\")\n       .setRatingCol(\"rating\")\n       .setPredictionCol(\"prediction\")\n       .setMaxIter(3)\n       .setSeed(seed)\n       .setRegParam(0.1)\n       .setColdStartStrategy(\"drop\")\n       .setRank(12)\n       .setNonnegative(True))\n\nalsModel = als.fit(trainingDF)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["Evaluate the model on the test data by looking at the mean squared error of our predictions on test data."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\n\nregEval = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"mse\")\n\npredictedTestDF = alsModel.transform(testDF)\n\ntestMse = regEval.evaluate(predictedTestDF)\n\nprint(f\"The model had a MSE on the test set of {testMse}\")"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["## Deep Learning\n\nNow let's take a deep learning approach to predicting the rating values.\n\nLet's take the latent factors learned from ALS and include them as features! The following cell extracts the user and item (movie) features from the trained ALS model and joins them in our train and test DataFrames."],"metadata":{}},{"cell_type":"code","source":["userFactors = alsModel.userFactors.selectExpr(\"id as userId\", \"features as uFeatures\")\nitemFactors = alsModel.itemFactors.selectExpr(\"id as movieId\", \"features as iFeatures\")\njoinedTrainDF = trainingDF.join(itemFactors, on=\"movieId\").join(userFactors, on=\"userId\")\njoinedTestDF = testDF.join(itemFactors, on=\"movieId\").join(userFactors, on=\"userId\")"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["display(joinedTrainDF)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["We would like to use the `iFeatures` and `uFeatures` columns as input for our deep learning model. However, we need all our features to be in one column of our DataFrame.\n\nThe code below creates two new DataFrames, `concatTrainDF` and `concatTestDF`, with the following three columns: `userId`, `movieId`, and `features` which contains the concatenated `iFeatures` and `uFeatures` arrays."],"metadata":{}},{"cell_type":"code","source":["from itertools import chain\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import ArrayType, FloatType\n\ndef concat_arrays(*args):\n    return list(chain(*args))\n    \nconcat_arrays_udf = udf(concat_arrays, ArrayType(FloatType()))\n\nconcatTrainDF = joinedTrainDF.select(\"userId\", \"movieId\", concat_arrays_udf(col(\"iFeatures\"), col(\"uFeatures\")).alias(\"features\"), \"rating\")\n\nconcatTestDF = joinedTestDF.select(\"userId\", \"movieId\", concat_arrays_udf(col(\"iFeatures\"), col(\"uFeatures\")).alias(\"features\"), \"rating\")"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["Check that your DataFrame has the correct columms."],"metadata":{}},{"cell_type":"code","source":["display(concatTrainDF.limit(10))"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["### Petastorm\n\nPrepare data for Petastorm."],"metadata":{}},{"cell_type":"code","source":["from petastorm.spark import SparkDatasetConverter, make_spark_converter\n\n# Clean up the directories if they exist\ndbutils.fs.rm(working_dir, recurse=True)\n\n# Define the paths\ntrain_path = f\"file://{working_dir}/ALS_train\"\ntest_path =  f\"file://{working_dir}/ALS_test\"\n\n# Create the required directories\ndbutils.fs.mkdirs(train_path)\ndbutils.fs.mkdirs(test_path)\n\n# Convert the Spark DF to TensorFlow datasets\nspark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, train_path)\nconverter_train = make_spark_converter(concatTrainDF.repartition(8).selectExpr(\"features\", \"rating as label\"))\n\n# Write test out to Parquet\nconcatTestDF.repartition(8).selectExpr(\"features\", \"rating as label\").write.mode(\"overwrite\").parquet(test_path)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["### Setup Model and HorovodRunner\n\nWe'll create two models, a baseline model that uses a linear activation function at the end, and one with a scaled [sigmoid](https://www.tensorflow.org/api_docs/python/tf/keras/backend/sigmoid) function whose outputs are bounded 0.5 to 5 (range of review scores)."],"metadata":{}},{"cell_type":"code","source":["import tensorflow as tf\ntf.random.set_seed(42)\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nimport numpy as np\nfrom tensorflow.keras import backend as K\nfrom petastorm import TransformSpec\n\ndef build_model():\n  return Sequential([Dense(30, input_shape=(24,), activation=\"relu\"),\n                     Dense(20, activation=\"relu\"),\n                     Dense(1, activation=\"linear\")])\n\ndef sigmoid_activation(x): # Scores range from 0.5 to 5\n  return (K.sigmoid(x) * 4.5) + .5\n\ndef build_model_sigmoid():\n  return Sequential([Dense(30, input_shape=(24,), activation=\"relu\"),\n                     Dense(20, activation=\"relu\"),\n                     Dense(1, activation=sigmoid_activation)])"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["### Tensorboard\nWe will set up Tensorboard so we can visualize and debug our network. Take a look at the [dbutils.tensorboard.start](https://docs.microsoft.com/en-us/azure/databricks/applications/deep-learning/single-node-training/tensorflow#--tensorboard) function. \n\nNOTE: It will not start logging until we start training our model."],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.rm(f\"{working_dir}/tensorboard\", True)\nlog_dir = f\"{working_dir}/tensorboard\"\ndbutils.tensorboard.start(log_dir)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["### HorvodRunner with Petastorm"],"metadata":{}},{"cell_type":"code","source":["from sparkdl import HorovodRunner\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.callbacks import *\nimport horovod.tensorflow.keras as hvd\n\nBATCH_SIZE = 64\nNUM_EPOCHS = 1\n\ndef run_training_horovod_petastorm(model_type=\"default\"):\n  # Horovod: initialize Horovod.\n  hvd.init()\n  print(f\"Rank is: {hvd.rank()}\")\n  print(f\"Size is: {hvd.size()}\")\n  with converter_train.make_tf_dataset(batch_size=BATCH_SIZE, num_epochs=None, cur_shard=hvd.rank(), shard_count=hvd.size()) as train_dataset:\n    dataset = train_dataset.map(lambda x: (x.features, x.label))  \n    if model_type == \"default\":\n      model = build_model()\n    else:\n      model = build_model_sigmoid()\n    \n    # Number of steps required to go through one epoch\n    steps_per_epoch = len(converter_train) // (BATCH_SIZE*hvd.size())\n    \n    # Horovod: adjust learning rate based on number of CPUs/GPUs.\n    optimizer = optimizers.Adam(lr=0.001*hvd.size())\n\n    # Horovod: add Horovod Distributed Optimizer.\n    optimizer = hvd.DistributedOptimizer(optimizer)\n\n    model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mse\"], experimental_run_tf_function=False)\n\n    # Use the optimized FUSE Mount\n    checkpoint_dir = f\"{working_dir}/{model_type}_horovod_checkpoint_weights.ckpt\"\n\n    callbacks = [\n      hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n      hvd.callbacks.MetricAverageCallback(),\n      hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=5, verbose=1),\n      ReduceLROnPlateau(monitor=\"loss\", patience=10, verbose=1)\n    ]\n\n    # Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\n    if hvd.rank() == 0:\n      callbacks.append(tf.keras.callbacks.ModelCheckpoint(checkpoint_dir))\n      callbacks.append(tf.keras.callbacks.TensorBoard(f\"{log_dir}/petastorm_{model_type}\"))\n\n    history = model.fit(dataset, callbacks=callbacks, steps_per_epoch=steps_per_epoch, epochs=NUM_EPOCHS)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["### Train and Evaluate Model with Linear Activation"],"metadata":{}},{"cell_type":"code","source":["hr = HorovodRunner(np=-1) # using all workers is very slow\nhr.run(run_training_horovod_petastorm, model_type=\"default\")"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["Load in saved model + test data"],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\nfrom tensorflow.keras.models import load_model\n\nmodel_type = \"default\"\nmodel = load_model(f\"{working_dir}/{model_type}_horovod_checkpoint_weights.ckpt\")\n\ntestDF = pd.read_parquet(test_path.replace(\"file://\", \"\"))\nX_test = np.array([np.array(row) for row in testDF[\"features\"].values]) # Reshape values\ny_test = testDF[\"label\"]"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["Let's evaluate and compare to how we did with vanilla ALS."],"metadata":{}},{"cell_type":"code","source":["# Get MSE loss of model\nloss, _ = model.evaluate(X_test, y_test)\nprint(f\"Default model loss: {loss}\")"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["### Train and Evaluate Model with Scaled Sigmoid\n\nNow train the model with the scaled sigmoid activation function by setting `model_type` to `sigmoid`."],"metadata":{}},{"cell_type":"code","source":["hr = HorovodRunner(np=-1) # using all workers is very slow\nhr.run(run_training_horovod_petastorm, model_type=\"sigmoid\")"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["How much better did the scaled sigmoid do?"],"metadata":{}},{"cell_type":"code","source":["model_type = \"sigmoid\"\nmodel = load_model(f\"{working_dir}/{model_type}_horovod_checkpoint_weights.ckpt\")\n\n# Get loss of model\nloss, _ = model.evaluate(X_test, y_test)\nprint(f\"Scaled sigmoid model loss: {loss}\")\n"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"DL 06b - Horovod ALS","notebookId":1391719663531330},"nbformat":4,"nbformat_minor":0}
