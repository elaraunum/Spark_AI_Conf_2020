{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# Multi-armed bandit\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lab you learn:<br>\n - One state MDPs also known as Multi-armed bandit\n - Try to understand the trade-off between exploration and exploitation\n  \n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) References\n* Sutton chapter 2"],"metadata":{}},{"cell_type":"markdown","source":["### Problem statement\n![Multi-armed bandit problem](https://files.training.databricks.com/images/rl/multiarm_bandit.png)\n<br/><br/><br/><br/>\nYou arrive in a casino. You decide to play slot machine. The slot machine has 10 levers. When you pull down a lever, some money is given to you. Your objective is to maximize your cumulative reward by picking different levers. This problem is known as **multi-armed bandit** problem. \n\n\n**How would you approach this problem? Discuss this with your neighbors. <br/>**\n\nIn this lab we try to answer this question."],"metadata":{}},{"cell_type":"markdown","source":["### Case 1: ###\n\n0. Set q(a) to random values for all levers \\\\(a = 1, 2, 3, ... , 10\\\\). q(a) can be thought of as an expected value of the reward for lever a. Since we do not know anything about the levers, we initialize their q values to 0.\n0. Take the first action by randomly picking one of the levers.\n0. Observe the reward.\n0. Calculate the expected value of the lever and update q for that lever.\n0. Pick a lever with the highest q value. If there are ties, randomly pick among one of the levers.\n0. Repeat 3-5 until step 1,000.\n0. Repeat 1-6 for 1000 different multi-armed bandit problems."],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nimport matplotlib.pyplot as plt\nnp.random.seed(1234)\n\n# Parameters of the distributions (for simulation)\nmu_q_star = 0\nsigma_q_star = 1\nlever_count = 10\nsigma_reward = 1\ntimesteps = 1000\nruns = 1000\nepsilon = 0.1\n\n\ndef qstar():\n  \"\"\"This function generates random mean values for levers.\"\"\"\n\n  return np.random.normal(mu_q_star, sigma_q_star, lever_count)\n\ndef reward(q_star, action):\n  \"\"\"This function generates rewards.\"\"\"\n  \n  return np.random.normal(q_star[action], sigma_reward, 1)\n\ndef plot_average_rewards(x, y):\n  \"\"\"This function plot the average rewards of 200 10-armed bandit problems.\"\"\"\n  \n  fig = plt.figure()\n  plt.plot(x, y)\n  fig.suptitle(r'Average reward per timestep', fontsize=20)\n  plt.xlabel('timestep', fontsize=18)\n  plt.ylabel('Average reward', fontsize=16)\n  plt.show()\n  display()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# ANSWER\ndef greedy():\n  \"\"\"This function runs multiple simulations for a 10-armed bandit problem.\"\"\"\n  \n  # Initialize a one-dimensional array with size timestep. This array includes the average rewards of 2000 runs for each cross section\n  cumulative_timestep_reward = np.zeros(timesteps)\n\n \n  \n  for i in range(runs):\n    print(f\"This is iteration {i+1}.\")\n    # Use qstar function defined above to initialize the mean of reward distribution.\n    q_star = qstar()\n    # Initialize q and count values to random int and zero, respectively.\n    q = np.zeros(lever_count)\n    \n    count = np.zeros(lever_count)\n    \n    # Randomly pick a lever\n    action = np.random.randint(lever_count, size= 1)\n    \n    # Initialize cumulative rewards and rewards\n    cumulative_rewards = np.zeros(lever_count)\n    rewards = np.zeros(timesteps)\n    \n    # For 1000 time steps run this simulation \n    for step in range (timesteps):\n      \n      # Keep track of number of times a lever has been picked.\n      count[action] += 1 \n      \n      # Observe the reward. Reward comes from a distribution defined earlier. \n      rewards[step] = reward(q_star, action)\n      # Calculate the cumulative \n      cumulative_rewards[action] = rewards[step]+cumulative_rewards[action]\n      \n      # Update the q values\n      q[action] = (cumulative_rewards[action])/count[action]\n      highest_q = np.argwhere(q == np.amax(q))\n      action = np.random.choice(highest_q.flatten(), 1)\n  \n    \n    \n    cumulative_timestep_reward += rewards\n      \n    \n  \n  return(cumulative_timestep_reward/runs)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["average_reward = greedy()\nx = np.linspace(1,timesteps, timesteps)\nplot_average_rewards(x, average_reward)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["### Case 2: ###\n\nNow assume that you want to do some exploration along with exploitation. Implement the following procedure and compare the results. \n\n0. Set q(a) to random values for all levers \\\\(a = 1, 2, 3, ... , 10\\\\). q(a) can be thought of as an expected value of the reward for lever a. Since we do not know anything about the levers, we initialize to 0.\n0. Take the first action by randomly picking one of the levers.\n0. Observe the reward\n0. Calculate the expected value of the lever and update q for that lever\n0. Pick a lever with highest q value with probability \\\\(1-\\epsilon\\\\) where \\\\(\\epsilon \\in [0,1]\\\\). Otherwise pick other levers.\n0. Repeat 3-5 until step 1,000.\n0. Repeat 1-6 for 1000 different multi-armed bandit problems.\n0. Repeat 1-7 for different values of \\\\(\\epsilon\\\\). \\\\(\\epsilon = 0, 0.01 , 0.1, 0.2, 0.5, 0.75, 1\\\\)"],"metadata":{}},{"cell_type":"code","source":["# ANSWER\ndef epsilon_greedy(epsilon):\n  \"\"\"This function implements e-greedy algorithm.\"\"\" \n  \n  # Set average_reward to zeros\n  cumulative_timestep_reward = np.zeros(timesteps)\n\n  for i in range(runs):\n    print(f\"This is iteration {i+1}.\")\n    # Use qstar function defined above to initialize the mean of reward distribution. \n    q_star = qstar()\n    # Initialize q and count values to random int and zero, respectively.\n    q = np.zeros(lever_count)\n    # Initialize count\n    count = np.zeros(lever_count)\n    # Randomly pick a lever\n    action = np.random.randint(lever_count, size= 1)\n    # Initialize the cumulative reward\n    cumulative_rewards = np.zeros(lever_count)\n    # Initialize the reward (for each run)\n    rewards = np.zeros(timesteps)\n    \n    for step in range (timesteps):\n      \n      # Increase the count for each lever\n      count[action] += 1\n      # Observe the reward\n      rewards[step] = reward(q_star, action)\n      # Calculate the cumulative reward\n      cumulative_rewards[action] = rewards[step]+cumulative_rewards[action]\n      # Update the q value\n      q[action] = (cumulative_rewards[action])/count[action]\n      # Act greedily by picking the lever with the highest q\n      highest_q = np.argwhere(q == np.amax(q))\n      \n      # Pick the lever that is NOT the best one with epsilon probability\n      if (np.random.uniform() < epsilon and len(highest_q) != len(q)):\n        not_highest_q = np.argwhere(q != np.amax(q))\n        action = np.random.choice(not_highest_q.flatten(), 1)\n       \n        \n      # Pick the lever with highest value of q with probability of 1-epsilon\n      else:\n        action = np.random.choice(highest_q.flatten(), 1)\n  \n\n    \n    cumulative_timestep_reward += rewards\n      \n    \n      \n  \n  return(cumulative_timestep_reward/runs)    "],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["average_reward = epsilon_greedy(epsilon)\nx = np.linspace(1,timesteps, timesteps)\nplot_average_rewards(x, average_reward)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["### Bonus: ###\n\nIn case 2 we had to accumulate all the previous rewards to update q values. Are there any more efficient methods to update q? Methods that are more memory efficient.<br/><br/><br/>\n\n**Hint:** <br\\>\n\n\\\\(\\begin{aligned} Q\\_{n+1} &= \\frac {\\sum \\_{i=1}^{i=n}R\\_i} {n} \\\\\\\n&= \\frac {1}{n}(R\\_{n} + \\sum \\_{i=1}^{i=n-1}R\\_i)\\\\\\\n&= \\frac {1}{n}(R\\_{n} + \\frac {n-1}{n-1} \\sum \\_{i=1}^{i=n-1}R\\_i)\\\\\\\n&= \\frac {1}{n}(R\\_{n} + (n-1)Q\\_{n})\\\\\\\n&= Q\\_{n} + \\frac {1}{n}[R\\_{n}-Q\\_{n}]\n\\end{aligned} \\\\)"],"metadata":{}},{"cell_type":"code","source":["# ANSWER\ndef epsilon_greedy_memory_efficient(epsilon):\n  \"\"\"This is implementation of above algorithm.\"\"\" \n  \n  cumulative_timestep_reward = np.zeros(timesteps)\n  \n  for i in range(runs):\n    print(f\"This is iteration {i+1}.\")\n    q_star = qstar()\n    q = np.zeros(lever_count)\n    count = np.zeros(lever_count)\n    action = np.random.randint(lever_count, size= 1)\n    rewards = np.zeros(timesteps)\n    \n    for step in range (timesteps):\n      count[action] += 1\n      rewards[step] = reward(q_star, action)\n      q[action] = q[action] + 1.0/count[action] * (rewards[step] - q[action])\n      highest_q = np.argwhere(q == np.amax(q))\n      \n      if (np.random.uniform() < epsilon and len(highest_q) != len(q)):\n        not_highest_q = np.argwhere(q != np.amax(q))\n        action = np.random.choice(not_highest_q.flatten(), 1)\n      \n      else:\n        action = np.random.choice(highest_q.flatten(), 1)\n  \n    \n   \n    cumulative_timestep_reward += rewards\n  \n  return(cumulative_timestep_reward/runs)  "],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["average_reward = epsilon_greedy_memory_efficient(epsilon)\nx = np.linspace(1,timesteps, timesteps)\nplot_average_rewards(x, average_reward)\n"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"RL 02Lb - MDP Lab","notebookId":2929930686998265},"nbformat":4,"nbformat_minor":0}
