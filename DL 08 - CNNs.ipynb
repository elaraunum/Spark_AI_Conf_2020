{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["#Convolutional Neural Networks\n\nWe will use pre-trained Convolutional Neural Networks (CNNs), trained with the image dataset from [ImageNet](http://www.image-net.org/), to demonstrate two aspects. First, how to explore and classify images. And second, how to use transfer learning with existing trained models (next lab).\n\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n - Analyze popular CNN architectures\n - Apply pre-trained CNNs to images using Pandas Scalar Iterator UDF"],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## VGG16\n![vgg16](https://neurohive.io/wp-content/uploads/2018/11/vgg16-neural-network.jpg)\n\nWe are going to start with the VGG16 model, which was introduced by Simonyan and Zisserman in their 2014 paper [Very Deep Convolutional Networks for Large Scale Image Recognition](https://arxiv.org/abs/1409.1556).\n\nLet's start by downloading VGG's weights and model architecture."],"metadata":{}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions, VGG16\nimport numpy as np\n\nvgg16Model = VGG16(weights=\"imagenet\")"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["We can look at the model summary. Look at how many parameters there are! Imagine if you had to train all 138,357,544 parameters from scratch! This is one motivation for re-using existing model weights.\n\n**RECAP**: What is a convolution? Max pooling?"],"metadata":{}},{"cell_type":"markdown","source":["**Question**: What do the input and output shapes represent?"],"metadata":{}},{"cell_type":"code","source":["vgg16Model.summary()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["## Inception-V3 + Batch Normalization\n\nIn 2016, developers from [Google published a paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf) updating their Inception architecture with a number of optimizations.  This included a technique known as batch normalization.\n\nEach layer in a deep neural network (with 10+ layers, for instance) expects the inputs from the previous layer to come from the same distribution.  However, in practice each layer is being updated, changing the distribution of its output to the next layer.  This is called \"internal covariate shift\" and can result in an unstable learning process since each layer is effectively learning a moving target. \n\n**Batch normalization is a technique that applies to very deep neural networks (especially CNNs) that standardizes the inputs to a layer for each mini-batch.** Generally speaking, this reduces the number of training epochs needed by stabilizing the learning process.\n\nBatch normalization should generally not be used with dropout, another regularization technique discussed in the Advanced Keras notebook.  While there's some contention over which is a more effective method ([see this paper for details](https://link.springer.com/article/10.1007/s11042-019-08453-9)), batch normalization is generally preferred over dropout for deep neural networks."],"metadata":{}},{"cell_type":"code","source":["from tensorflow.keras.applications.inception_v3 import InceptionV3\n\ninceptionModel = InceptionV3()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Take a look at the architecture noted where batch normalization is performed."],"metadata":{}},{"cell_type":"code","source":["inceptionModel.summary()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["Looking for more reference architectures?  Check out [`tf.keras.applications` for what's available out of the box.](https://www.tensorflow.org/api_docs/python/tf/keras/applications)"],"metadata":{}},{"cell_type":"markdown","source":["## Apply pre-trained model\n\nWe are going to make a helper method to resize our images to be 224 x 224, and output the top 3 classes for a given image.\n\nIn TensorFlow, it represents the images in a channels-last manner: (samples, height, width, color_depth)"],"metadata":{}},{"cell_type":"code","source":["def predict_images(images, model):\n  for i in images:\n    print(f\"Processing image: {i}\")\n    img = image.load_img(i, target_size=(224, 224))\n    # Convert to numpy array for Keras image format processing\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = preprocess_input(x)\n    preds = model.predict(x)\n    # Decode the results into a list of tuples (class, description, probability)\n    print(f\"Predicted: {decode_predictions(preds, top=3)[0]}\\n\")"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["-sandbox\n## Images\n<div style=\"text-align: left; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://files.training.databricks.com/images/pug.jpg\" height=\"150\" width=\"150\" alt=\"Databricks Nerds!\" style=>\n  <img src=\"https://files.training.databricks.com/images/strawberries.jpg\" height=\"150\" width=\"150\" alt=\"Databricks Nerds!\" style=>\n  <img src=\"https://files.training.databricks.com/images/rose.jpg\" height=\"150\" width=\"150\" alt=\"Databricks Nerds!\" style=>\n  \n</div>\n\nLet's make sure the datasets are already mounted."],"metadata":{}},{"cell_type":"code","source":["img_paths = [\n  \"/dbfs/mnt/training/dl/img/pug.jpg\", \n  \"/dbfs/mnt/training/dl/img/strawberries.jpg\", \n  \"/dbfs/mnt/training/dl/img/rose.jpg\"\n]\n\npredict_images(img_paths, vgg16Model)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["The network did so well with the pug and strawberry! What happened with the rose? Well, it turns out that `rose` was not one of the 1000 categories that VGG16 had to predict. But it is quite interesting it predicted `sea_anemone` and `vase`."],"metadata":{}},{"cell_type":"markdown","source":["You can play around with this with your own images by doing the following:\n\nGet a new file: \n\n`%sh wget image_url.jpg`\n\n`%fs cp file:/databricks/driver/image_name.jpg yourName/tmp/image_name.jpg `\n\n\nOR\n\nYou can upload this file via the Data UI and read in from the FileStore path (e.g. `/dbfs/FileStore/image_name.jpg`)."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n## Classify Co-Founders of Databricks\n<div style=\"text-align: left; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://files.training.databricks.com/images/Ali-Ghodsi-4.jpg\" height=\"150\" width=\"150\" alt=\"Databricks Nerds!\" style=>\n  <img src=\"https://files.training.databricks.com/images/andy-konwinski-1.jpg\" height=\"150\" width=\"150\" alt=\"Databricks Nerds!\" style=>\n  <img src=\"https://files.training.databricks.com/images/ionS.jpg\" height=\"150\" width=\"150\" alt=\"Databricks Nerds!\" style=>\n  <img src=\"https://files.training.databricks.com/images/MateiZ.jpg\" height=\"200\" width=\"150\" alt=\"Databricks Nerds!\" style=>\n  <img src=\"https://files.training.databricks.com/images/patrickW.jpg\" height=\"150\" width=\"150\" alt=\"Databricks Nerds!\" style=>\n  <img src=\"https://files.training.databricks.com/images/Reynold-Xin.jpg\" height=\"150\" width=\"150\" alt=\"Databricks Nerds!\" style=>\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["Load these images into a DataFrame."],"metadata":{}},{"cell_type":"code","source":["df = spark.read.format(\"image\").load(\"/mnt/training/dl/img/founders/\")\ndf.cache().count()\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["Let's wrap the prediction code inside a UDF so we can apply this model in parallel on each row of the DataFrame."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import StringType, ArrayType\n\n# Helper func to return top3 results as strings in an array\ndef get_results(path, model, preprocess_input, decode_predictions, target_size=(224,224)):\n  img = image.load_img(path, target_size=target_size)\n  x = image.img_to_array(img)\n  x = np.expand_dims(x, axis=0)\n  x = preprocess_input(x)\n  preds = model.predict(x)\n\n  # Decode the results into a list of tuples (class, description, probability)  \n  top_3 = decode_predictions(preds, top=3)[0]\n  result = []\n  for _, label, prob in top_3:\n    result.append(f\"{label}: {prob:.3f}\")\n  return result\n\n# Define UDF to do preprocessing and prediction steps\n@udf(ArrayType(StringType()))\ndef vgg16_predict_udf(image_data):\n  path = image_data[0].replace(\"dbfs:\", \"/dbfs\")\n  model = VGG16(weights=\"imagenet\")\n  return get_results(path, model, preprocess_input, decode_predictions)\n\ndisplay(df.withColumn(\"Top 3 VGG16 Predictions\", vgg16_predict_udf(\"image\")))"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["### Vectorized UDF\n\nAs of Spark 2.3, there are Vectorized UDFs available in Python to help speed up the computation.\n\n* [Blog post](https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html)\n* [Documentation](https://spark.apache.org/docs/latest/sql-programming-guide.html#pyspark-usage-guide-for-pandas-with-apache-arrow)\n\n<img src=\"https://databricks.com/wp-content/uploads/2017/10/image1-4.png\" alt=\"Benchmark\" width =\"500\" height=\"1500\">\n\nVectorized UDFs utilize Apache Arrow to speed up computation. Let's see how that helps improve our processing time.\n\nThe user-defined functions are executed by: \n* [Apache Arrow](https://arrow.apache.org/), is an in-memory columnar data format that is used in Spark to efficiently transfer data between JVM and Python processes with near-zero (de)serialization cost. See more [here](https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html).\n* pandas inside the function, to work with pandas instances and APIs."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Pandas Scalar Iterator UDF\n\nIf you define your own UDF to apply a model to each record of your DataFrame in Python, opt for vectorized UDFs for optimized serialization and deserialization. However, if your model is very large, then there is high overhead for the Pandas UDF to repeatedly load the same model for every batch in the same Python worker process. In Spark 3.0, Pandas UDFs can accept an iterator of pandas.Series or pandas.DataFrame so that you can load the model only once instead of loading it for every series in the iterator.\n\nThis way the cost of any set-up needed (like loading the VGG16 model in our case) will be incurred fewer times. When the number of images youâ€™re working with is greater than `spark.conf.get('spark.sql.execution.arrow.maxRecordsPerBatch')`, which is 10,000 by default, you'll see significant speed ups over a pandas scalar UDF because it iterates through batches of pd.Series.\n\nIt has the general syntax of: \n```@pandas_udf(...)\ndef predict(iterator):\n  model = ... # load model\n  for features in batch_iter:\n    yield model.predict(features)```\n\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> If the workers cached the model weights after loading it for the first time, subsequent calls of the same UDF with the same model loading will become significantly faster."],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\nfrom pyspark.sql.functions import pandas_udf\nfrom typing import Iterator\n\ndef preprocess(image_path):\n  path = image_path.replace(\"dbfs:\", \"/dbfs\")\n  img = image.load_img(path, target_size=(224, 224))\n  x = image.img_to_array(img)\n  x = preprocess_input(x)\n  return x\n\n@pandas_udf(ArrayType(StringType()))\ndef vgg16_predict_pandas_udf(image_data_iter: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n  # Load model outside of for loop\n  model = VGG16(weights=\"imagenet\") \n  for image_data_series in image_data_iter:\n    image_path_series = image_data_series[\"origin\"]\n    # Apply functions to entire series at once\n    x = image_path_series.map(preprocess) \n    x = np.stack(list(x.values))\n    preds = model.predict(x, batch_size=6)\n    top_3s = decode_predictions(preds, top=3)\n    \n    # Format results\n    results = []\n    for top_3 in top_3s:\n      result = []\n      for _, label, prob in top_3:\n        result.append(f\"{label}: {prob:.3f}\")\n      results.append(result)\n    yield pd.Series(results)\n\ndisplay(df.withColumn(\"Top 3 VGG16 Predictions (pandas udf)\", vgg16_predict_pandas_udf(\"image\")))"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["Yikes! These are not the most natural predictions (because ImageNet did not have a `person` category). In the next lab, we will cover how to utilize existing components of the VGG16 architecture, and how to retrain the final classifier."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"DL 08 - CNNs","notebookId":1391719663531259},"nbformat":4,"nbformat_minor":0}
