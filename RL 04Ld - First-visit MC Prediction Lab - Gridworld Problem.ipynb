{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# First-visit MC Prediction - Gridworld Problem\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lab you learn:<br>\n - Policy Evaluation"],"metadata":{}},{"cell_type":"markdown","source":["### GridWorld Problem V2 ###\nConsider the following environment:\n\n1. There is a 4 by 4 grid.\n2. The goal is to reach top left or bottom right of the grid.\n3. There are 4 actions: UP, DOWN, LEFT and RIGHT. Each action results in a move. The reward for each action is -1 until you reach the terminal points. The reward for the terminal points is 0. If you are at the edge, you do not move to a new state; however, you are given a reward. For example, if you are at the top right corner and decide to go RIGHT, you will end up at the same place and the reward is given to you.\n4. We have already made your job easier by creating the environment for such a problem. Familiarize yourself with this environment [here]($./helper/GridWorldEnvironment).\n\n<br>\n![gridenv](https://files.training.databricks.com/images/rl/gridenv.png)\n\n### Problem Statement ###\n\nIn this lab we are going to evaluate a policy in a full RL problem setting i.e. we do NOT know the dynamic of the environment nor are we given the MDP. We are going to use already-built environment to develop **MC first-visit** algorithm to evaluate a random policy."],"metadata":{}},{"cell_type":"code","source":["%run \"./helper/GridWorldEnvironment\""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["environment = GridWorldEnvironment()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["#ANSWER\nimport random\nimport numpy as np\nfrom statistics import mean\n\ndef monte_carlo(number_episodes):\n  \"\"\"This function generates multiple episodes for the gridworld problem.\"\"\"\n  \n  np.random.seed(1234)\n  # Initial transition list. We will add to this list as we create new ones. In the end, we end up with list of the list.\n  visited_states = []\n  immediate_rewards = []\n  \n  # Create samples of episodes\n  for i in range (number_episodes):\n    \n    # Randomly pick the starting point\n    start_index = random.randint(1,14)\n    \n    # Beginning of the episode. empty lists.\n    realized_states = []\n    realized_rewards = []\n    realized_states.append(start_index)\n    environment.set_state(start_index)\n    while True:\n      # Randomly pick an action. Remember this is a random policy\n      action = random.randint(0,3)\n      # Observe the state, reward, whether or not we have reached the terminal points\n      next_state, reward, is_done, _= environment.step(action)\n     \n      # Keep the immediate reward\n      realized_rewards.append(reward)\n      \n      # Leave if we are in the end\n      if is_done:\n        break\n      # Record the next state\n      realized_states.append(next_state)\n      start_index = next_state\n    \n    # Add the list to the final list. visited_states is the list of list. each list contains one of the episodes. \n    # Immediate_rewards is list of list. each list contains the immediate rewards.\n    visited_states.append(realized_states)\n    immediate_rewards.append(realized_rewards)\n    \n  return visited_states, immediate_rewards\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["#ANSWER\ndef mc_first_visit(visited_states,immediate_rewards):\n  \"\"\"This function gets the list of episodes and list of immediate rewards associated with each episodes.\"\"\"\n  \n  # Initialize \n  cumulative_rewards = np.zeros(16)\n  cumulative_counts = np.ones(16)\n  j = 0\n  \n  for episodes in visited_states:\n    # Find first occurrences of each element\n    first_occurrences = sorted(episodes.index(states) for states in set(episodes))\n    for i in first_occurrences:\n      # Calculate the cumulative reward and count for each episode\n      cumulative_rewards[episodes[i]] += np.sum(immediate_rewards[j][i:])\n      cumulative_counts[episodes[i]] += 1 \n    j += 1\n      \n  return cumulative_rewards, cumulative_counts"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["visited_states, immediate_rewards = monte_carlo(100000)\ncumulative_rewards, cumulative_counts = mc_first_visit(visited_states,immediate_rewards)\nvalue = cumulative_rewards/cumulative_counts"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# Test your code\nvalue_expected = [0.0, -14.0, -19.9, -21.0, -14.0, -18.0, -19.9, -19.0, -20.0, -19.0, -17.0, -13.0, -21.0, -19.0, -13.0, 0.0 ]\nnp.testing.assert_array_almost_equal(value, value_expected, err_msg = \"The values are incorrect\", decimal = 0)\n"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"RL 04Ld - First-visit MC Prediction Lab - Gridworld Problem","notebookId":2929930686998369},"nbformat":4,"nbformat_minor":0}
