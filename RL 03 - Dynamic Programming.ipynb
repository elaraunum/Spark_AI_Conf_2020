{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# Dynamic Programming \n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you learn:<br>\n - Policy Evaluation\n - Policy Iteration\n - Value Iteration\n - Proof of convergence (contraction mapping) if time permits\n<br>\n\n### Out of scope\n- Extension to Dynamic Programming is out of scope\n  \n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) References\n* [David Silver lecture](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ)\n* Sutton book - Chapter 4"],"metadata":{}},{"cell_type":"markdown","source":["### Definitions ###\n<br>\n0. **Dynamic Programming:** It is a method for solving complex problems. In a nutshell, we solve problems by breaking them down into subproblems:\n - Solve subproblems\n - Stitch the solutions together to solve the bigger problem\n0. **Dynamic** means the problem has a dynamic to it. It changes over time or it is sequential. \n - Examples?\n0. **Programming** means finding optimal solution to a problem (\"program\"). Optimal solution in RL means optimal policy. \n - The terminology is used in other places like \"linear programming\"."],"metadata":{}},{"cell_type":"markdown","source":["### Where is Dynamic Programming applicable? ###\nTwo conditions **must** be met in order to use Dynamic Programming: \n\n<br>\n0. Optimal substructure\n - Principle of optimality: every optimal policy consists only of optimal sub policies\n0. Overlapping subproblems\n - Subproblems reoccur many times\n - Solutions can be reused\n - i.e. It has a recursive structure\n<br>\n\n### Question: ###\n\nIs Dynamic Programming applicable to MDPs? If so, why? If not, why not?"],"metadata":{}},{"cell_type":"markdown","source":["### Model-based RL ###\n\nWe showed in previous lessons that MDPs satisfy both properties. MDPs have recursive structures and they can be decomposed into subproblems. See **\\\\(v\\_{\\*}\\\\)** and **\\\\(q\\_{\\*}\\\\)** calculations in previous notebook. In this notebook, we discuss Model-based Reinforcement Learning. This means that:\n0. We have the full knowledge of the MDP. i.e. we know how world works, the transition probabilities, full knowledge of environment etc. \n0. We are doing **planning**:\n - We know the MDP or MRP and the policy, we evaluate the policy\n - This is **prediction** problem \n0. We know MDP, we find optimal value function and optimal policy\n - This is a **control** problem\n\nJust note that Dynamic Programming has a lot of applications:\n0. Scheduling algorithm\n0. Pricing algorithm\n0. And many more"],"metadata":{}},{"cell_type":"markdown","source":["### Prediction problem (i.e. evaluate a policy) using Model-based RL ###\n\nHow would you evaluate the policy? Discuss it with your neighbors. Hint: Look at the Bellman equation\n0. Initialize \\\\(v\\_{s} \\forall s\\in S\\\\) \n0. At each iteration for all state\n0. For all states \\\\(s \\in S\\\\)\n0. \\\\(v\\_{k+1}(s)\\longleftarrow v\\_{k}(s')\\\\) where \\\\(s'\\\\) is a successor state of \\\\(s\\\\)\n\nReminder: \n\n\\\\( \\begin{aligned} \\\\\\ v^{k+1} &= \\sum\\_{a\\in A} \\pi(a\\bigm\\vert s) (R^a\\_{s}+\\gamma \\sum\\_{s'\\in S} \\rho\\_{ss'}^a v\\_{\\pi}(s') \\\\\\\n&= R^{\\pi} + \\gamma \\rho^{\\pi}v^{k} \\end{aligned}\\\\)\n\nNote: This is synchronous backups. All states are updated at the same time. There is another way of doing it asynchronously. See me if you want some guidance on it.\n\n### Question ###\n\nHow do you know the iterations converge? Is the convergence guaranteed?"],"metadata":{}},{"cell_type":"markdown","source":["### How do you improve the policy (policy iterations)? ###\n\n0. Iterative process\n - Evaluate the policy \\\\(\\pi \\\\). $$ v\\_{\\pi}(s) = E[R\\_{t+1} + \\gamma R\\_{t+2}+ ... \\bigm\\vert S\\_{t} = s] $$\n - Improve the policy by refining the policy. i.e. when you do 1, update the policy then go back to 1. $$ \\pi' = greed(v\\_{\\pi}) $$\n \n0. You can start with deterministic policy and act greedy to achieve the optimal policy (talk to me if you need a proof)\n0. Multiple extensions:\n - Do you have to do one iteration one improvement and so on? NO\n - You can simply do multiple iterations and one updates\n - You can have early stopping criterion \n - ANY policy evaluation + ANY policy improvements work!"],"metadata":{}},{"cell_type":"markdown","source":["### Principle of Optimality ###\n\nA policy achieved the optimal value from state s **if and only if** for any state \\\\(s'\\\\) reachable from s, the policy achieves the optimal value from state \\\\(s'\\\\). i.e. an optimal policy consists of two components:\n\n<br\\>\n\n0. An optimal first action\n0. Followed by an optimal policy from successor state \\\\(s'\\\\)\n\n\n### Questions ###\n\n0. How is this applicable to DP? Hint: Consider solution \\\\(v\\_{*}(s)\\\\) formula. Start from final rewards and work backward\n0. Consider a 5 by 5 grid. The goal is to find the shortest path to the upper left square on the grid. Can you apply the principal of optimality to find the value of each square?"],"metadata":{}},{"cell_type":"markdown","source":["### Value Iteration ### \n\n0. Iterative process\n0. \\\\(v\\_{1}\\longrightarrow v\\_{2}\\longrightarrow v\\_{3}\\longrightarrow v\\_{4}...\\longrightarrow v\\_{*}\\\\)\n0. Synchronous backups i.e. all states are updated at the same time\n0. Convergence is guaranteed (talk to me if you need a proof)\n0. There is no policy iteration (only value)\n\n**Note:** Intermediate value functions may not correspond to any policy"],"metadata":{}},{"cell_type":"markdown","source":["### Summary ###\n<br>\n\n0. We are given full knowledge of environment and MDP (planning problem)\n0. We can:\n  - Do prediction by doing policy evaluation\n  - Do control by doing policy iteration\n  - Do control by doing value iteration\n0. We discussed synchronous updates in which all states are updated in parallel\n0. Algorithm complexity: \\\\(O(mn^2)\\\\) per iteration where m is number of actions and n is number of states\n0. Could instead calculate action-value function. What is the complexity for such calculation?"],"metadata":{}},{"cell_type":"markdown","source":["### Some extensions ###\nYou can run algorithms in asynchronous manner i.e each state is updated individually in any order\n\n  - Reduce computation \n  - Guaranteed convergence **if all states** are selected continually. We want to make sure all states are updated enough times\n  - **In-place DP, Prioritized sweeping and Real-time DP**"],"metadata":{}},{"cell_type":"markdown","source":["### In-place DP\n<br>\n - Only save one copy of value function. i.e. no need to update all of the state at the same time. Do it sequentially. You can use the new value of the state to update the next state\n - Instead of $$v\\_{new}(s) \\longleftarrow max\\_{a\\in A} \\Bigg( R\\_{s}^a + \\gamma \\sum\\_{s' \\in S}\\rho\\_{ss'}^a v\\_{old}(s')\\Bigg) $$\n $$ v\\_{old}(s) \\longleftarrow v\\_{new}(s)  $$\n Do $$v(s) \\longleftarrow max\\_{a\\in A} \\Bigg( R\\_{s}^a + \\gamma \\sum\\_{s' \\in S}\\rho\\_{ss'}^a v(s')\\Bigg) $$"],"metadata":{}},{"cell_type":"markdown","source":["### Prioritized sweeping ###\n<br>\n - Calculate the error $$\\Bigg\\vert max\\_{a\\in A} \\Bigg( R\\_{s}^a + \\gamma \\sum\\_{s' \\in S}\\rho\\_{ss'}^a v(s')\\Bigg)-v(s) \\Bigg\\vert $$\n - Update the state with highest error\n - Repeat"],"metadata":{}},{"cell_type":"markdown","source":["### Real-time DP ###\n\n<br>\n - Only update the states that matters to the agent\n - Run the agent (algorithm) and update the state that agent is experiencing"],"metadata":{}},{"cell_type":"markdown","source":["### Some thoughts on Dynamic Programming ###\n\n0. We require to do full-width back up. i.e. We need to go through all possible actions and state to update a state\n0. We must know the dynamic of the environment and MDP\n0. Consider when you have millions of state and millions of actions. DP is not a right approach. \n0. It is good for medium-sized problems\n0. Number of states grows exponentially as number of state variables increases\n\n### Some thoughts on possible solutions ###\n\n0. Sample a trajectory. i.e sample the dynamic \\\\(\\langle S, A, R, S'\\rangle \\\\)\n0. No need to know dynamic of environment nor the full knowledge of the MDP\n0. This results in a model-free approach (more to come)\n0. Not as expensive as Dynamic Programming. The cost is not dependent on the number of state variables\n\n### Approximate Dynamic Programming ###\n0. Use function approximator such as NN\n0. Sample states, calculate the state-values\n0. Train the function approximator"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"RL 03 - Dynamic Programming","notebookId":2929930686998048},"nbformat":4,"nbformat_minor":0}
