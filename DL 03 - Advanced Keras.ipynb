{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# Advanced Keras\n\nCongrats on building your first neural network! In this notebook, we will cover even more topics to improve your model building. After you learn the concepts here, you will apply them to the neural network you just created.\n\nWe will use the California Housing Dataset.\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n - Perform data standardization for better model convergence\n - Add validation data\n - Generate model checkpointing/callbacks\n - Use TensorBoard\n - Apply dropout regularization"],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["from sklearn.datasets.california_housing import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\n\ncal_housing = fetch_california_housing()\n\n# split 80/20 train-test\nX_train, X_test, y_train, y_test = train_test_split(cal_housing.data,\n                                                    cal_housing.target,\n                                                    test_size=0.2,\n                                                    random_state=1)\n\nprint(cal_housing.DESCR)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n.. _california_housing_dataset:\n\nCalifornia Housing dataset\n--------------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 20640\n\n    :Number of Attributes: 8 numeric, predictive attributes and the target\n\n    :Attribute Information:\n        - MedInc        median income in block\n        - HouseAge      median house age in block\n        - AveRooms      average number of rooms\n        - AveBedrms     average number of bedrooms\n        - Population    block population\n        - AveOccup      average house occupancy\n        - Latitude      house block latitude\n        - Longitude     house block longitude\n\n    :Missing Attribute Values: None\n\nThis dataset was obtained from the StatLib repository.\nhttp://lib.stat.cmu.edu/datasets/\n\nThe target variable is the median house value for California districts.\n\nThis dataset was derived from the 1990 U.S. census, using one row per census\nblock group. A block group is the smallest geographical unit for which the U.S.\nCensus Bureau publishes sample data (a block group typically has a population\nof 600 to 3,000 people).\n\nIt can be downloaded/loaded using the\n:func:`sklearn.datasets.fetch_california_housing` function.\n\n.. topic:: References\n\n    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n      Statistics and Probability Letters, 33 (1997) 291-297\n\n</div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["Let's take a look at the distribution of our features."],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\n\npd.DataFrame(X_train, columns=cal_housing.feature_names).describe()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MedInc</th>\n      <th>HouseAge</th>\n      <th>AveRooms</th>\n      <th>AveBedrms</th>\n      <th>Population</th>\n      <th>AveOccup</th>\n      <th>Latitude</th>\n      <th>Longitude</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>16512.000000</td>\n      <td>16512.000000</td>\n      <td>16512.000000</td>\n      <td>16512.000000</td>\n      <td>16512.000000</td>\n      <td>16512.000000</td>\n      <td>16512.000000</td>\n      <td>16512.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>3.876149</td>\n      <td>28.604469</td>\n      <td>5.441114</td>\n      <td>1.099598</td>\n      <td>1425.257146</td>\n      <td>3.094971</td>\n      <td>35.632194</td>\n      <td>-119.574288</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.891584</td>\n      <td>12.586046</td>\n      <td>2.613727</td>\n      <td>0.507173</td>\n      <td>1123.756792</td>\n      <td>11.597402</td>\n      <td>2.137087</td>\n      <td>2.007578</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.499900</td>\n      <td>1.000000</td>\n      <td>0.846154</td>\n      <td>0.333333</td>\n      <td>3.000000</td>\n      <td>0.750000</td>\n      <td>32.540000</td>\n      <td>-124.300000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>2.572050</td>\n      <td>18.000000</td>\n      <td>4.439906</td>\n      <td>1.006260</td>\n      <td>786.000000</td>\n      <td>2.427283</td>\n      <td>33.930000</td>\n      <td>-121.810000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>3.544550</td>\n      <td>29.000000</td>\n      <td>5.226528</td>\n      <td>1.048797</td>\n      <td>1164.000000</td>\n      <td>2.813449</td>\n      <td>34.260000</td>\n      <td>-118.490000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>4.750000</td>\n      <td>37.000000</td>\n      <td>6.057778</td>\n      <td>1.099574</td>\n      <td>1723.000000</td>\n      <td>3.273834</td>\n      <td>37.710000</td>\n      <td>-118.010000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>15.000100</td>\n      <td>52.000000</td>\n      <td>141.909091</td>\n      <td>34.066667</td>\n      <td>35682.000000</td>\n      <td>1243.333333</td>\n      <td>41.950000</td>\n      <td>-114.310000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["## 1. Data Standardization\n\nBecause our features are all on different scales, it's going to be more difficult for our neural network during training. Let's do feature-wise standardization.\n\nWe are going to use the [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) from Sklearn, which will remove the mean (zero-mean) and scale to unit variance.\n\n$$x' = \\frac{x - \\bar{x}}{\\sigma}$$"],"metadata":{}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["## Keras Model"],"metadata":{}},{"cell_type":"code","source":["import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense  \ntf.random.set_seed(42)\n\nmodel = Sequential([\n  Dense(20, input_dim=8, activation=\"relu\"),\n  Dense(20, activation=\"relu\"),\n  Dense(1, activation=\"linear\")\n])\n\n#creating linear stack of layers\n##each layer has their own AF"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["from tensorflow.keras.optimizers import Adam\n\nmodel.compile(optimizer=Adam(lr=0.01), loss=\"mse\", metrics=[\"mse\"])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["## 2. Validation Data\n\nLet's take a look at the [.fit()](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#fit) method in the docs to see all of the options we have available! \n\nWe can either explicitly specify a validation dataset, or we can specify a fraction of our training data to be used as our validation dataset.\n\nThe reason why we need a validation dataset is to evaluate how well we are performing on unseen data (neural networks will overfit if you train them for too long!).\n\nWe can specify `validation_split` to be any value between 0.0 and 1.0 (defaults to 0.0)."],"metadata":{}},{"cell_type":"code","source":["history = model.fit(X_train, y_train, validation_split=.2, epochs=10, verbose=2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Epoch 1/10\n413/413 - 1s - loss: 0.6822 - mse: 0.6822 - val_loss: 0.4361 - val_mse: 0.4361\nEpoch 2/10\n413/413 - 1s - loss: 0.3911 - mse: 0.3911 - val_loss: 0.3821 - val_mse: 0.3821\nEpoch 3/10\n413/413 - 1s - loss: 0.3647 - mse: 0.3647 - val_loss: 0.3767 - val_mse: 0.3767\nEpoch 4/10\n413/413 - 1s - loss: 0.3620 - mse: 0.3620 - val_loss: 0.3699 - val_mse: 0.3699\nEpoch 5/10\n413/413 - 1s - loss: 0.3490 - mse: 0.3490 - val_loss: 0.3643 - val_mse: 0.3643\nEpoch 6/10\n413/413 - 1s - loss: 0.3408 - mse: 0.3408 - val_loss: 0.3558 - val_mse: 0.3558\nEpoch 7/10\n413/413 - 1s - loss: 0.3286 - mse: 0.3286 - val_loss: 0.3634 - val_mse: 0.3634\nEpoch 8/10\n413/413 - 1s - loss: 0.3263 - mse: 0.3263 - val_loss: 0.3380 - val_mse: 0.3380\nEpoch 9/10\n413/413 - 1s - loss: 0.3157 - mse: 0.3157 - val_loss: 0.4185 - val_mse: 0.4185\nEpoch 10/10\n413/413 - 1s - loss: 0.3235 - mse: 0.3235 - val_loss: 0.3265 - val_mse: 0.3265\n</div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["## 3. Checkpointing\n\nAfter each epoch, we want to save the model. However, we will pass in the flag `save_best_only=True`, which will only save the model if the validation loss decreased. This way, if our machine crashes or we start to overfit, we can always go back to the \"good\" state of the model.\n\nTo accomplish this, we will use the ModelCheckpoint [callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint). History is an example of a callback that is automatically applied to every Keras model."],"metadata":{}},{"cell_type":"code","source":["from tensorflow.keras.callbacks import ModelCheckpoint\n\nfilepath = f\"{working_dir}/keras_checkpoint_weights.ckpt\"\n\nmodel_checkpoint = ModelCheckpoint(filepath=filepath, verbose=1, save_best_only=True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"code","source":["working_dir"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[12]: &#39;/dbfs/user/odl_user_195841@databrickslabs.onmicrosoft.com/deep_learning/dl_03_advanced_keras&#39;</div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["## 4. Tensorboard\n\n[Tensorboard](https://www.tensorflow.org/tensorboard/get_started) provides a nice UI to visualize and debug your neural networks. We can also define it as a callback."],"metadata":{}},{"cell_type":"code","source":["from tensorflow.keras.callbacks import TensorBoard\n\nlog_dir = f\"{working_dir}/_tb.dir\"\ntensorboard = TensorBoard(log_dir)\n\ndbutils.tensorboard.start(log_dir) # Will be empty until call .fit() below\n\n#dbutils is a databricks utility"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["\n<html>\n    <head>\n        <link rel=\"stylesheet\"type=\"text/css\"\n        href=\"https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css\" />\n    </head>\n    <body>\n        TensorBoard log directory set to: /dbfs/user/odl_user_195841@databrickslabs.onmicrosoft.com/deep_learning/dl_03_advanced_keras/_tb.dir. <a href=/driver-proxy/o/6990469333701502/0620-200051-swore503/9009/>\n            View TensorBoard <i class=\"fa fa-external-link\"> </i>\n        </a>\n    </body>\n</html>\n"]}}],"execution_count":18},{"cell_type":"markdown","source":["Now let's add in our model checkpoint and Tensorboard callbacks to our `.fit()` command."],"metadata":{}},{"cell_type":"code","source":["history = model.fit(X_train, y_train, validation_split=.2, epochs=10, verbose=2, callbacks=[model_checkpoint, tensorboard])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Epoch 1/10\nWARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.116791). Check your callbacks.\n\nEpoch 00001: val_loss improved from inf to 0.33837, saving model to /dbfs/user/odl_user_195841@databrickslabs.onmicrosoft.com/deep_learning/dl_03_advanced_keras/keras_checkpoint_weights.ckpt\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nINFO:tensorflow:Assets written to: /dbfs/user/odl_user_195841@databrickslabs.onmicrosoft.com/deep_learning/dl_03_advanced_keras/keras_checkpoint_weights.ckpt/assets\n413/413 - 2s - loss: 0.3072 - mse: 0.3072 - val_loss: 0.3384 - val_mse: 0.3384\nEpoch 2/10\n\nEpoch 00002: val_loss improved from 0.33837 to 0.31146, saving model to /dbfs/user/odl_user_195841@databrickslabs.onmicrosoft.com/deep_learning/dl_03_advanced_keras/keras_checkpoint_weights.ckpt\nINFO:tensorflow:Assets written to: /dbfs/user/odl_user_195841@databrickslabs.onmicrosoft.com/deep_learning/dl_03_advanced_keras/keras_checkpoint_weights.ckpt/assets\n413/413 - 2s - loss: 0.3094 - mse: 0.3094 - val_loss: 0.3115 - val_mse: 0.3115\nEpoch 3/10\n\nEpoch 00003: val_loss did not improve from 0.31146\n413/413 - 1s - loss: 0.3033 - mse: 0.3033 - val_loss: 0.3224 - val_mse: 0.3224\nEpoch 4/10\n\nEpoch 00004: val_loss did not improve from 0.31146\n413/413 - 1s - loss: 0.3035 - mse: 0.3035 - val_loss: 0.3233 - val_mse: 0.3233\nEpoch 5/10\n\nEpoch 00005: val_loss did not improve from 0.31146\n413/413 - 1s - loss: 0.3028 - mse: 0.3028 - val_loss: 0.3291 - val_mse: 0.3291\nEpoch 6/10\n\nEpoch 00006: val_loss did not improve from 0.31146\n413/413 - 1s - loss: 0.2967 - mse: 0.2967 - val_loss: 0.3177 - val_mse: 0.3177\nEpoch 7/10\n\nEpoch 00007: val_loss did not improve from 0.31146\n413/413 - 1s - loss: 0.3166 - mse: 0.3166 - val_loss: 0.3390 - val_mse: 0.3390\nEpoch 8/10\n\nEpoch 00008: val_loss did not improve from 0.31146\n413/413 - 1s - loss: 0.2964 - mse: 0.2964 - val_loss: 0.3152 - val_mse: 0.3152\nEpoch 9/10\n\nEpoch 00009: val_loss did not improve from 0.31146\n413/413 - 1s - loss: 0.2902 - mse: 0.2902 - val_loss: 0.3888 - val_mse: 0.3888\nEpoch 10/10\n\nEpoch 00010: val_loss did not improve from 0.31146\n413/413 - 1s - loss: 0.2894 - mse: 0.2894 - val_loss: 0.3116 - val_mse: 0.3116\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":["model.evaluate(X_test, y_test)\n\n##validation loss is a bit higher than training. Model may be overfit"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\r  1/129 [..............................] - ETA: 0s - loss: 0.1623 - mse: 0.1623\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 55/129 [===========&gt;..................] - ETA: 0s - loss: 0.3014 - mse: 0.3014\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r108/129 [========================&gt;.....] - ETA: 0s - loss: 0.3049 - mse: 0.3049\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r129/129 [==============================] - 0s 929us/step - loss: 0.3052 - mse: 0.3052\nOut[17]: [0.30519285798072815, 0.30519285798072815]</div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["-sandbox\n## 5. Dropout Regularization\n\nIt's generally more difficult to overtrain neural networks than classical machine learning methods.  However, overfitting is more common with smaller datasets and is caused in part by co-adapted neurons.\n\n**Dropout is a regularization method that reduces overfitting by randomly and temporarily removing nodes during training.**  It works like this:<br><br>\n\n- Apply to most type of layers (e.g. fully connected, convolutional, recurrent) and larger networks\n- Set an additional probability for keeping each node\n  - .5 is a good starting place for hidden layers\n  - .9 is a good starting place for the input layer\n- Temporarily and randomly remove nodes and their connections during each training cycle\n- Since this results in larger weights, scale the weights proportional to the dropout rate\n- Score on the entire architecture (without dropout)\n\n![](https://files.training.databricks.com/images/nn_dropout.png)\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> See the original paper here: [Dropout: A Simple Way to Prevent Neural Networks from\nOverfitting](http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)"],"metadata":{}},{"cell_type":"markdown","source":["Redefine a larger network.  Note the following changes:<br><br>\n\n- We'll use a dropout rate of .5\n- Increase the size of the network in proportion to the dropout rate (`original network size` / `dropout rate`)\n- Large weight sizes can be a sign of an unstable network.  Manage this using a weight constraint to force the magnitude of all weights to be below a specific value.  Typical values for this constraint `c` are between 3 and 4."],"metadata":{}},{"cell_type":"code","source":["import tensorflow as tf\nfrom tensorflow.keras.constraints import max_norm\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\ntf.random.set_seed(42)\n\ndef create_dropout_model():\n  model = Sequential()\n  model.add(Dropout(.1))\n  model.add(Dense(40, input_dim=8, activation=\"relu\", kernel_constraint=max_norm(4))) #use between 3 or 4 as rule of thumb\n  model.add(Dropout(.5))#dropping 50% of units\n  model.add(Dense(40, activation=\"relu\", kernel_constraint=max_norm(4)))\n  model.add(Dropout(.5))\n  model.add(Dense(1, activation=\"linear\"))\n  return model\n\n#drop out can come before or after adding units\n\ndropoutModel = create_dropout_model()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":24},{"cell_type":"markdown","source":["Compile the model with a learning rate increased by 1-2 orders of magnitude.\n\n> \"Although dropout alone gives significant improvements, using dropout along with maxnorm regularization, large decaying learning rates and high momentum provides a significant\nboost over just using dropout\"\n> [- Dropout: A Simple Way to Prevent Neural Networks from\nOverfitting](http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)"],"metadata":{}},{"cell_type":"code","source":["dropoutModel.compile(optimizer=Adam(lr=0.1), loss=\"mse\", metrics=[\"mse\"]) \n\ndropoutHistory = dropoutModel.fit(X_train, y_train, validation_split=.2, epochs=10, verbose=2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Epoch 1/10\nWARNING:tensorflow:Layer dropout is casting an input tensor from dtype float64 to the layer&#39;s dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it&#39;s dtype defaults to floatx.\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx(&#39;float64&#39;)`. To change just this layer, pass dtype=&#39;float64&#39; to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\n413/413 - 1s - loss: 1.6315 - mse: 1.6315 - val_loss: 1.3789 - val_mse: 1.3789\nEpoch 2/10\n413/413 - 1s - loss: 1.3550 - mse: 1.3550 - val_loss: 1.3439 - val_mse: 1.3439\nEpoch 3/10\n413/413 - 1s - loss: 1.3470 - mse: 1.3470 - val_loss: 1.3537 - val_mse: 1.3537\nEpoch 4/10\n413/413 - 1s - loss: 1.3606 - mse: 1.3606 - val_loss: 1.3437 - val_mse: 1.3437\nEpoch 5/10\n413/413 - 1s - loss: 1.3470 - mse: 1.3470 - val_loss: 1.3485 - val_mse: 1.3485\nEpoch 6/10\n413/413 - 1s - loss: 1.3502 - mse: 1.3502 - val_loss: 1.3589 - val_mse: 1.3589\nEpoch 7/10\n413/413 - 1s - loss: 1.3470 - mse: 1.3470 - val_loss: 1.3746 - val_mse: 1.3746\nEpoch 8/10\n413/413 - 1s - loss: 1.3490 - mse: 1.3490 - val_loss: 1.3421 - val_mse: 1.3421\nEpoch 9/10\n413/413 - 1s - loss: 1.5685 - mse: 1.5685 - val_loss: 1.3375 - val_mse: 1.3375\nEpoch 10/10\n413/413 - 1s - loss: 1.3442 - mse: 1.3442 - val_loss: 1.3355 - val_mse: 1.3355\n</div>"]}}],"execution_count":26},{"cell_type":"markdown","source":["-sandbox\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Batch normalization is an alternative approach.  This will be discussed in the lesson on CNNs."],"metadata":{}},{"cell_type":"markdown","source":["Now it's your turn to try out these techniques on the Boston Housing Dataset!"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"DL 03 - Advanced Keras","notebookId":1391719663531289},"nbformat":4,"nbformat_minor":0}
