{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# Keras\n\nIn this notebook, we will build upon the concepts introduced in the previous lab to build a neural network that is more powerful than a simple linear regression model!\n\nWe will use the California Housing Dataset.\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n  - Will modify these parameters for increased model performance:\n    - Activation functions\n    - Loss functions\n    - Optimizer\n    - Batch Size\n  -  Save and load models"],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["from sklearn.datasets.california_housing import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\n\ncal_housing = fetch_california_housing()\n\n# split 80/20 train-test\nX_train, X_test, y_train, y_test = train_test_split(cal_housing.data,\n                                                    cal_housing.target,\n                                                    test_size=0.2,\n                                                    random_state=1)\n\nprint(cal_housing.DESCR)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["-sandbox\n\n## Recall from Last Lab\n\n##### Steps to build a Keras model\n<img style=\"width:20%\" src=\"https://files.training.databricks.com/images/5_cycle.jpg\" >"],"metadata":{}},{"cell_type":"markdown","source":["## Define a Network\n\nLet's not just reinvent linear regression. Let's build a model, but with multiple layers using the [Sequential model](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) from Keras.\n\n![](https://files.training.databricks.com/images/Neural_network.svg)"],"metadata":{}},{"cell_type":"markdown","source":["## 1. Activation Function\n\nIf we keep the activation as linear, then we aren't utilizing the power of neural networks!! The power of neural networks derives from the non-linear combinations of linear functions.\n\n**RECAP:** So what are our options for [activation functions](http://cs231n.github.io/neural-networks-1/#actfun)?"],"metadata":{}},{"cell_type":"code","source":["from tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\nimport tensorflow as tf\ntf.random.set_seed(42)\n\nmodel = Sequential()\n\n# Input layer\nmodel.add(Dense(20, input_dim=8, activation=\"relu\")) \n\n# Automatically infers the input_dim based on the layer before it\nmodel.add(Dense(20, activation=\"relu\")) \n\n# Output layer\nmodel.add(Dense(1, activation=\"linear\")) "],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["#### Alternative Keras Model Syntax"],"metadata":{}},{"cell_type":"code","source":["def build_model():\n  return Sequential([Dense(20, input_dim=8, activation=\"relu\"),\n                     Dense(20, activation=\"relu\"),\n                     Dense(1, activation=\"linear\")]) # Keep the last layer as linear because this is a regression problem"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["We can check the model definition by calling `.summary()`"],"metadata":{}},{"cell_type":"code","source":["model = build_model()\nmodel.summary()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["## 2. Loss Functions + Metrics\n\nIn Keras, the *loss function* is the function for our optimizer to minimize. *[Metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)* are similar to a loss function, except that the results from evaluating a metric are not used when training the model.\n\n**Recap:** Which loss functions should we use for regression? Classification?"],"metadata":{}},{"cell_type":"code","source":["from tensorflow.keras import metrics\nfrom tensorflow.keras import losses\n\nloss = \"mse\" # Or loss = losses.mse\nmetrics = [\"mae\", \"mse\"] # Or metrics = [metrics.mae, metrics.mse]\n\nmodel.compile(optimizer=\"sgd\", loss=loss, metrics=metrics)\nmodel.fit(X_train, y_train, epochs=10)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["## 3. Optimizer\n\nWOW! We got a lot of NANs! Let's try this again, but using the Adam optimizer. There are a lot of optimizers out there, and here is a [great blog post](http://ruder.io/optimizing-gradient-descent/) illustrating the various optimizers.\n\nWhen in doubt, the Adam optimizer does a very good job. If you want to adjust any of the hyperparameters, you will need to import the optimizer from `optimizers` instead of passing in the name as a string."],"metadata":{}},{"cell_type":"code","source":["from tensorflow.keras import optimizers\n\nmodel = build_model()\noptimizer = optimizers.Adam(lr=0.001)\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\nhistory = model.fit(X_train, y_train, epochs=20)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n\ndef viewModelLoss():\n  plt.clf()\n  plt.plot(history.history[\"loss\"])\n  plt.title(\"Model Loss\")\n  plt.ylabel(\"Loss\")\n  plt.xlabel(\"Epoch\")\n  plt.show()\n  \nviewModelLoss()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["## 4. Batch Size\n\nLet's set our `batch_size` (how much data to be processed simultaneously by the model) to 64, and increase our `epochs` to 20. Mini-batches are often a power of 2, to facilitate memory allocation on GPU (typically between 16 and 512).\n\n\nAlso, if you don't want to see all of the intermediate values print out, you can set the `verbose` parameter: 0 = silent, 1 = progress bar, 2 = one line per epoch (defaults to 1)"],"metadata":{}},{"cell_type":"code","source":["model = build_model()\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\nhistory = model.fit(X_train, y_train, epochs=20, batch_size=64, verbose=2)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["## 5. Evaluate"],"metadata":{}},{"cell_type":"code","source":["model.evaluate(X_test, y_test)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["## 6. Save Model, Load Model and Train More\n\nWhenever you train neural networks, you want to save them. This way, you can reuse them later! \n\nIn our case, we want to save  need to save both the architecture and the weights, so we will use `model.save`. If you only want to save the weights, you can use `model.save_weights`."],"metadata":{}},{"cell_type":"code","source":["filepath = f\"{working_dir}/keras_checkpoint_weights.ckpt\"\n\nmodel.save(filepath)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["You can load both the model and architecture together using `load_model()`"],"metadata":{}},{"cell_type":"code","source":["from tensorflow.keras.models import load_model\n\nnew_model = load_model(filepath)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["Check that the model architecture is the same."],"metadata":{}},{"cell_type":"code","source":["new_model.summary()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["Let's train it for one more epoch (we need to recompile), and then save those weights.  This is a *warm start.*"],"metadata":{}},{"cell_type":"code","source":["new_model.compile(optimizer=\"adam\", loss=\"mse\")\nnew_model.fit(X_train, y_train, validation_split=.2, epochs=1, verbose=2)\nnew_model.save_weights(filepath)\n"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"DL 02 - Keras","notebookId":1391719663530978},"nbformat":4,"nbformat_minor":0}
