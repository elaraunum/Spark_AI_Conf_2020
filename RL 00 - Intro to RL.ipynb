{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# Introduction to Reinforcement Learning\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you learn:<br>\n - Types of Machine Learning problems\n - Reinforcement Learning problem\n - Agent\n - Environment\n - RL vocabulary\n - RL shortcomings\n \n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Audience\n* Primary Audience: This course is ideal for data scientists that are interested to learn about next-level algorithms\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Prerequisites\n* Web browser: **Chrome**\n* A cluster configured with **8 cores** and **DBR 7.0 ML**\n* Experience with Python, numpy, and pandas is required\n* Familiarity with Probability Theory, Linear Algebra, and Machine Learning is required\n* Suggested Courses from <a href=\"https://academy.databricks.com/\" target=\"_blank\">Databricks Academy</a>:\n  - DB 096 - Just Enough Python for Apache Sparkâ„¢\n  \n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) References\n* [David Silver lecture](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ)\n* Sutton book - Chapter 1"],"metadata":{}},{"cell_type":"markdown","source":["### Where does RL fit in Machine Learning?\n<br/><br/>\n![RL in ML](https://files.training.databricks.com/images/rl/rl.png)\n<br>\n### RL's Interaction with Other Fields\n![RL application](https://files.training.databricks.com/images/rl/RL_application.png)"],"metadata":{}},{"cell_type":"markdown","source":["### RL's Characteristics\n<br>\n0. Trial and error (like a child to learn how to walk)\n0. Sequential process, so time matters (non i.i.d)\n0. Current decision affects later outcome\n0. Reward signal\n\n### Examples:\n\n0. [Helicopter](https://www.youtube.com/watch?v=VCdxqn0fcnE)\n0. [Atari game](https://www.youtube.com/watch?v=V1eYniJ0Rnk)"],"metadata":{}},{"cell_type":"markdown","source":["### Reinforcement Learning Setup\n<br>\n![RL agent and environment](https://files.training.databricks.com/images/rl/RL_agent_env.png)\n\n**At each time step \\\\(t\\\\) agent:**\n0. Receive observation \\\\(O\\_{t}\\\\)\n0. Take an action \\\\(A\\_{t}\\\\) i.e. decision made by an agent (algorithm).\n0. Receive reward: \\\\(R\\_{t}\\\\) is a scalar value. See it as a feedback signal. Indicates how good agent is doing.\n\n**At each time step \\\\(t\\\\) environment:**\n0. Receive an action \\\\(A{_t}\\\\) i.e. decision made by an agent (algorithm)\n0. Emits observation \\\\(O_{t+1}\\\\): response to agent's action.\n0. Emits reward: \\\\(R_{t+1}\\\\) is a scalar value. See it as a feedback signal. Indicates how good agent is doing\n\n**RL assumption: All goals can be described by the maximization of expected cumulative reward**\n\n### Exercise ###\n- Discuss some examples of agent, environment, action, observation, reward?\n\n**Things to keep in mind:**\n0. What actions should we take at each step?\n0. Long vs. short term impact of an action?\n0. Should we wait or take immediate best action?\n0. Reward might be delayed? Examples?"],"metadata":{}},{"cell_type":"markdown","source":["### History and State\n<br>\n0. All observable variables up to time \\\\(t\\\\)\n    * \\\\(H\\_{t} =  O\\_{0}, A\\_{0}, R\\_{1}, O\\_{1}, A\\_{1}, R\\_{2},..., O\\_{t-1}, A\\_{t-1}, R\\_{t}\\\\)\n0. What happens next depends on history. How?\n0. **State** is the information used to determine what happens next (depends on the history)\n    * \\\\(S_t = f(H_t)\\\\)\n0. Agent's vs. environment's state (\\\\(S_t^a\\\\) and \\\\(S_t^e\\\\)): the state representation of agent and environment, respectively\n0. Most of the time environment state is not visible\n0. **Information state** contains all useful information from history. The state is **Markov** if future is independent of the past given the present. i.e we only care about the last state.\n$$ P[ S\\_{t+1} \\bigm\\vert S\\_{t} ] = P[ S\\_{t+1} \\bigm\\vert S\\_{1},S\\_{2},...,S\\_{t} ] $$\n0. Examples of Markov state? \n    * \\\\(S_t^e\\\\)?\n    * \\\\(H_t\\\\)?"],"metadata":{}},{"cell_type":"markdown","source":["### State definition matters!\n<br>\n![rat, cheese, lever](https://files.training.databricks.com/images/rl/rat_cheese_lever.jpg)"],"metadata":{}},{"cell_type":"markdown","source":["### Fully vs. Partially Observable Environments \n<br>\n0. Fully: Agent directly observes the environment state. Formally, this is a Markov Decision Process. \n\n    * Examples?\n    * \\\\(O\\_{t} = S\\_{t}^a = S\\_{t}^e\\\\)\n0. Partial observation: Agent indirectly observes environment. \n\n    * Requires memory: \n     0. \\\\(S\\_{t}^a = H_{t}\\\\)\n     0. \\\\(S\\_{t}^a = (P [(S\\_{t}^e=s^1 ],...,P[S\\_{t}^e=s^n ] )\\\\)\n     0. RNN \n     0. Examples?\n     \nNote: Remainder of this course focuses on **Markov Decision Process**."],"metadata":{}},{"cell_type":"markdown","source":["### Components of an RL Agent \n<br>\n0. Policy: agent's behavior \n    * Deterministic : \\\\(\\pi(s)\\\\)\n    * Stochastic: \\\\(\\pi(a \\bigm\\vert s) = P[A_t = a \\bigm\\vert S_t = s]\\\\)\n0. Value function: How good is each state/action? Thinks of it as expected value of all future rewards given the current state.\n    * \\\\(v\\_\\pi(s) = E\\_\\pi[R\\_{t+1} + \\gamma R\\_{t+2} + \\gamma^2R\\_{t+3}+... \\bigm\\vert S_t = s] \\\\)\n0. Model: agent's representation of the environment\n\n    * Predicts what environment will do next \n    * Predict the next state \\\\(\\rho\\_{ss'}^a = P[S_{t+1} = s' \\bigm\\vert S_t = s, A_t = a]\\\\) \n    * Predict the next reward \\\\(R\\_{s}^a = E[R_{t+1} \\bigm\\vert S_t = s, A_t = a]\\\\)"],"metadata":{}},{"cell_type":"markdown","source":["### Exercise\n<br>\nConsider the following grid. The arrows indicate the policy.\n0. What are the states?\n0. Given the policy (arrows) find the value of each state? \n    * Assume reward is -1 per time step\n    * Assume the value of terminal state is 0\n    * Assume \\\\(\\gamma = 1\\\\)\n0. What is the agent representation of the environment i.e. what is the model? What is the reward?\n<br><br>\n<img src=\"https://files.training.databricks.com/images/rl/maze.png\" width=\"700\">"],"metadata":{}},{"cell_type":"markdown","source":["### Types of RL agents\n![RL Taxonomy](https://files.training.databricks.com/images/rl/rl_taxonomy.png)"],"metadata":{}},{"cell_type":"markdown","source":["### Types of problems in sequential decision making\n<br>\n1. RL\n    - We do not know the environment initially\n    - Agent <> Environment interaction\n    - Iterated to improve the policy\n    - Example?\n2. Planning\n    - Model of environment is known\n    - Agent computes, because it has the model, without any interaction\n    - Agent improves the policy\n    - Example?"],"metadata":{}},{"cell_type":"markdown","source":["### Exploration vs. Exploitation\n<br>\n1. RL is trial-and-error learning. It is similar to how a child learns to walk\n2. How does an agent learn a good policy?\n    - Its past experiences. It uses information it learned from the environment to maximize the reward. This is called **exploitation**.\n        * Example?\n    - New experiences. It needs to find more information about the environment. This is called **exploration**.\n        * Example?"],"metadata":{}},{"cell_type":"markdown","source":["### Prediction vs. Control\n0. Prediction problem's goal: given a policy, what is the value function? \n0. Control problem's goals: find the best policy and accordingly optimal value function over all possible policies.\n0. Example on gridworld problem. <br><br><br><br>\n![Prediction](https://files.training.databricks.com/images/rl/prediction.png)<br><br><br><br>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"RL 00 - Intro to RL","notebookId":2929930686998455},"nbformat":4,"nbformat_minor":0}
