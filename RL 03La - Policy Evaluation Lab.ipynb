{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# Dynamic Programming: Policy Evaluation Lab\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lab you learn:<br>\n - Policy Evaluation\n  \n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) References\n* Sutton Chapter 2"],"metadata":{}},{"cell_type":"markdown","source":["### Policy Evaluation ###\n<br>\nIn this lab, we are going to evaluate a policy. Keep in mind that we are assuming that the MDP is given and we know the dynamic of the environment i.e. we are going to solve a **planning problem**. Consider the following grid, we created an environment for this problem earlier. We are going to use that environment and create an agent. Follow the following steps to evaluate a **random policy**.\n\n0. Initialize all states with value 0. Set \\\\(\\gamma = 0.9 \\\\). What does that mean to pick \\\\(\\gamma = 0.9 \\\\)? \n0. Assume a random policy i.e. 25% chance of going to right, left, up and down no matter where you are. The state does not change if the agent is going out of the grid. However, the reward is given.\n0. Update the state values based on above formula for two iterations i.e. k = 0, 1, 2, 3. What happens to terminal states' values? Do they change?\n0. Observe how policy changes through iterations\n0. Write above algorithm in Python\n\n![gridenv](https://miro.medium.com/max/1000/1*iX-Fu5YzUZ8CNEZ86BvfKA.png)"],"metadata":{}},{"cell_type":"markdown","source":["Recreate the environment for the Dynamic Programming from the first lab."],"metadata":{}},{"cell_type":"code","source":["%run \"./helper/GridWorldAdvancedEnvironment\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["environment = GridWorldAdvancedEnvironment()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# ANSWER \nimport copy\n\ndef evaluate_policy(environment, gamma=0.9, iterations=200):\n    \"\"\"Evaluate a policy given the full information of the environment. This is not the full RL.\"\"\"\n    \n    # Initialize the number of time visited a state \n    # Initialize the values of each state. We are doing synchronous updates. We need to keep the old values. \n    count = 0\n    new_values = np.zeros(environment.ns)\n    old_values = np.zeros(environment.ns)\n    \n    # Action's probability at any given state\n    action_probability = 0.25\n    \n    # Probability of going to state s' from s if you take an action\n    probability = 1\n    \n    while count < iterations:\n      for state in range(environment.ns):\n        # Make sure we cover all states\n        environment.set_state(state)\n        #environment.reset(state)\n        # Use the formula to back up the values\n        value = 0\n        for i in range(4):\n          # Take an action\n          next_state, reward, is_done, _= environment.step(i)\n          # Use the formula to back up the values\n          value = value + action_probability * (reward + gamma * probability * old_values[next_state])\n          environment.set_state(state)\n          #environment.reset(state)\n        \n        # Update the values     \n        new_values[state] = value\n      # Increase the count by 1\n      count += 1 \n      # Keep the copy of old values\n      old_values = copy.deepcopy(new_values)\n    return np.array(new_values)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# ANSWER\ninitial_value = evaluate_policy(environment, gamma=0.9, iterations=0)\none_iteration_value = evaluate_policy(environment, gamma=0.9, iterations=1)\ntwo_iterations_value = evaluate_policy(environment, gamma=0.9, iterations=2)\nthree_iterations_value = evaluate_policy(environment, gamma=0.9, iterations=3)\ntwo_hundred_iterations_value = evaluate_policy(environment, gamma=0.9, iterations=200)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["print(f\"Initial state value:{initial_value}\")\nprint(f\"State value after 1 iteration:{one_iteration_value}\")\nprint(f\"State value after 2 iterations:{two_iterations_value}\")\nprint(f\"State value after 3 iteration:{three_iterations_value}\")\nprint(f\"State value after 200 iteration::{two_hundred_iterations_value}\")"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# Test your code\nvalue_expected = np.array([3.30, 8.78,  4.42,  5.32,  1.49,  1.52, 2.99, 2.25, 1.90, 0.54, 0.05, 0.73, 0.67, 0.35, -0.40, -0.97, -0.43, -0.35, -0.58, -1.18, -1.85, -1.34, -1.22, -1.42, -1.97])\nnp.testing.assert_array_almost_equal(two_hundred_iterations_value, value_expected, err_msg= \"The values are incorrect\", decimal=2)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["### Questions ###\n0. Calculate: \n - \\\\(q(0,up)\\\\)\n - \\\\(q(1,down)\\\\)\n - \\\\(q(5,left)\\\\)\n0. How does the policy change in each iteration?\n0. What is the optimal policy?\n0. What are the state values under the given policy?"],"metadata":{}},{"cell_type":"code","source":["# ANSWER\n\n# Answer to question 1\nprint(f\"q(0, up) is {2.3} \")\nprint(f\"q(1, down) is {8.7} \")\nprint(f\"q(5, left) is {19.4} \")\n\n# Answer to question 2\n# The policy evolves to optimal policy. To see this in action, run it for 1 iteration, 2 iterations, 3 iterations and 4 iterations and see how values are evolving\n\n# Answer to question 3\n# The optimal policy is the stable one at the end. You try to move towards higher values\n\n# Answer to question 4\nprint(f\"State values are {two_hundred_iterations_value}\" )\n"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"RL 03La - Policy Evaluation Lab","notebookId":2929930686998291},"nbformat":4,"nbformat_minor":0}
