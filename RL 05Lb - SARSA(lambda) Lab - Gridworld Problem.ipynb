{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# SARSA(\\\\(\\lambda\\\\)) Control - Gridworld Problem\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lab you learn:<br>\n - Finding Optimal Policy Using SARSA(\\\\(\\lambda\\\\))"],"metadata":{}},{"cell_type":"markdown","source":["### Problem Statement ###\n\nIn this lab we are going to find an optimal policy. Keep in mind that we do NOT know the dynamic of the environment nor are we given the MDP i.e. this is the full RL prediction problem. We created an environment for this gridworld problem earlier. We are going to use that environment to develop SARSA algorithm to find optimal policy.\n\n![Prediction](https://files.training.databricks.com/images/rl/prediction.png)"],"metadata":{}},{"cell_type":"code","source":["%run \"./helper/GridWorldEnvironment\""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["environment = GridWorldEnvironment()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["#ANSWER\nimport numpy as np\nimport random\nnp.random.seed(1234)\n\n\ndef pick_action(Q, na, epsilon=0.1):\n  \"\"\"This function picks the greedy action according to e-greedy algorithm.\"\"\"\n  \n  actions = set([0,1,2,3])\n  \n  # Pick the greedy action with probability greater than epsilon/na \n  if np.random.rand() >= epsilon/na :\n    return np.argmax(Q)\n  # Pick a non-greedy action with a probability of epsilon/na \n  else:\n    return random.sample(actions - set([np.argmax(Q)]), 1)[0]\n      \n\ndef sarsa_lambda(gamma=1.0, epsilon=0.1, alpha=0.1, number_of_iterations=200000, lambda_=0.5):\n  '''\n  This function implements SARSA algorithm.\n  input:\n  gamma: discount factor\n  epsilon: e-greedy \n  alpha: learning rate\n  number_of_iterations: number of times to run the simulation\n  lambda_ = lambda used in SARSA(lambda) algorithm\n  output: optimal policy. Array of state/action pairs\n  '''\n  \n  # Number of states and actions\n  ns = 16\n  na = 4\n  # Number of time steps\n  time_step = 1000\n  # Initialize Q(S,A). Set them to zero \n  Q = np.zeros([ns, na])\n  # Initialize E(S,A). Set them to zero\n  E = np.zeros([ns, na])\n \n  # Create samples of episodes\n  for i in range(number_of_iterations):\n    if i%10000 == 0:\n      print(f\"This is iteration {i}\")\n    # Initial start point\n    start_state_index = random.randint(1,14)\n    action_index = pick_action(Q[start_state_index][:], na)\n    environment.set_state(start_state_index)\n    for j in range(time_step):\n      # Take an action and observe next state, reward and whether or not we have reached the terminal points.\n      next_state, reward, is_done, _ = environment.step(action_index)\n      # Take next action according to e-greedy algorithm\n      next_action_index = pick_action(Q[next_state][:], na)\n      # Error\n      delta = reward + gamma * Q[next_state][next_action_index] - Q[start_state_index][action_index]\n      # Eligibility trace \n      E[start_state_index][action_index] = gamma * lambda_ * E[start_state_index][action_index] + 1\n      # Update the Q\n      Q[start_state_index][action_index] = Q[start_state_index][action_index] + alpha * delta * E[start_state_index][action_index]\n      \n      # Leave the loop if at terminal points\n      if is_done:\n        break\n      start_state_index = next_state\n      action_index = next_action_index\n      \n    \n  return Q\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["Q = sarsa_lambda()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# Test your code\nvalue_expected = [[ 0.0,  0.0,  0.0,  0.0],\n       [-2.0, -3.0, -3.0, -1.0],\n       [-3.0, -4.1, -4.0, -2.0],\n       [-4.1, -4.1, -3.3, -3.1],\n       [-1.0, -3.0, -3.0, -2.0],\n       [-2.0, -4.0, -4.0, -2.3],\n       [-3.3, -3.0, -3.3, -3.2],\n       [-4.1, -3.0, -2.0, -4.0],\n       [-2.0, -4.0, -4.0, -3.0],\n       [-3.2, -3.3, -3.1, -3.2],\n       [-4.0, -2.0, -2.2, -4.0],\n       [-3.0, -2.0, -1.0, -3.0],\n       [-3.2, -3.0, -4.1, -4.1],\n       [-4.0, -2.0, -3.2, -4.0],\n       [-3.1, -1.0, -2.0, -3.0],\n       [ 0.0,  0.0,  0.0,  0.0]]  \nnp.testing.assert_array_almost_equal(Q, value_expected, err_msg = \"The values are wrong\", decimal = 0)\n"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"RL 05Lb - SARSA(lambda) Lab - Gridworld Problem","notebookId":2929930686998249},"nbformat":4,"nbformat_minor":0}
