{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# Model-Free Control  \n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you learn:<br>\n - On-Policy MC Control\n - On-Policy TD Learning\n - Off-Policy Learning\n  \n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) References\n* [David Silver lecture](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ)\n* Sutton book - Chapter 5, 6, 7, 8"],"metadata":{}},{"cell_type":"markdown","source":["### Knowledge check ###\n- What is the difference between Control and Prediction problems?\n- Some examples of problems what can be modeled with MDPs?\n- In what cases do you use Model-free control?"],"metadata":{}},{"cell_type":"markdown","source":["### On and Off-Policy Learning ###\n0. On-policy learning\n - You learn about policy \\\\(\\pi \\\\) from the experience sampled from \\\\(\\pi \\\\). In other words, you learn from policy \\\\(\\pi \\\\) by taking actions based on policy \\\\(\\pi\\\\)\n0. Off-policy learning\n - You learn about policy \\\\(\\pi \\\\) from experience sampled from \\\\(\\mu\\\\)"],"metadata":{}},{"cell_type":"markdown","source":["### Questions (refresher) ###\n<br>\n0. What was policy iteration?\n0. Can you apply that method using MC? Why?"],"metadata":{}},{"cell_type":"markdown","source":["### Questions###\n<br>\n0. What is the solution?"],"metadata":{}},{"cell_type":"markdown","source":["### \\\\(\\epsilon-Greedy\\\\) Exploration ###\n0. Simple idea to explore the space\n0. If you have m actions, you take one of them with probability of \\\\(\\frac{\\epsilon}{m}\\\\). i. e\n$$ \\pi(a|s) = \\begin{cases}\n   \\frac{\\epsilon}{m} + 1 - \\epsilon &\\text{if } a^{\\* } = argmax\\_{a\\in A}   Q(s,a)\\\\\\\n   \\frac{\\epsilon}{m} &\\text{otherwise}\n\\end{cases} $$"],"metadata":{}},{"cell_type":"markdown","source":["### Some points to keep in mind ###\n- \\\\(\\epsilon-greedy\\\\) policy improves over time (proof out of scope. Talk to me)\n- **For any \\\\(\\epsilon-greedy\\\\) policy \\\\(\\pi\\\\), the \\\\(\\epsilon-greedy\\\\) policy \\\\(\\pi'\\\\) with respect to \\\\(q\\_{\\pi}\\\\) is an improvement, \\\\(V\\_{pi}' \\ge V\\_{\\pi}(s)\\\\)** (proof out of scope, talk to me if you need a proof)\n- You do not have to iterate through multiple episodes to update the policy. One can take on episode or two, update Q(s,a), then apply \\\\(\\epsilon-greedy\\\\) then repeat"],"metadata":{}},{"cell_type":"markdown","source":["### Definition ###\n- Greedy in the Limit with Infinite Exploration (GLIE)\n - All state-action pairs are explored infinitely many times, $$lim\\_{k\\rightarrow \\infty} N\\_{k}(s,a)=\\infty$$\n - The policy converges on a greedy policy, $$ lim\\_{k \\rightarrow \\infty} \\pi\\_{k}(a\\bigm\\vert s) = 1 )a = argmax\\_{a\\in A} Q\\_{k}(s, a')$$\n- \\\\(\\epsilon-greedy\\\\) is GLIE if \\\\(\\epsilon \\\\) goes to zero as we progress for example choose \\\\(\\epsilon\\_{k} = \\frac{1}{k}\\\\)"],"metadata":{}},{"cell_type":"markdown","source":["### GLIE MC Control ###\n0. Sample kth episode using the policy \\\\(\\pi\\\\)\n0. For ** every episode **\n - For each state \\\\(S\\_{t}\\\\) and \\\\(A\\_{t}\\\\) in the episode,\n - \\\\(N\\big(S\\_{t}, A\\_{t}) \\longleftarrow N\\big(S\\_{t}, A\\_{t}) + 1  \\\\), where \\\\(N\\big(S\\_{t}, A\\_{t})\\\\) is the number of times that \\\\(\\big(S\\_{t}, A\\_{t}\\big)\\\\) is appeared.\n - \\\\(Q\\big(S\\_{t}, A\\_{t})\\longleftarrow Q\\big(S\\_{t}, A\\_{t} \\big) + \\frac{1}{N\\big(S\\_{t}, A\\_{t}\\big )} (G\\_{t} -Q(S\\_{t}, A\\_{t})\\big) \\\\),\n - \\\\(\\epsilon \\longleftarrow \\frac{1}{k}\\\\),\n - \\\\(\\pi \\longleftarrow \\epsilon-greedy (Q)\\\\)\n0. If we do this long enough, \\\\(Q\\big(s,a)\\longrightarrow q\\_{\\*}(s,a)\\\\)"],"metadata":{}},{"cell_type":"markdown","source":["### Refresher ###\n- What are the advantages of TD over MC?\n\n### Questions ###\n- How can you use TD instead of MC for a Control problem?"],"metadata":{}},{"cell_type":"markdown","source":["### Convergence of SARSA ###\n- Sarsa converges to the optimal action-value function, \\\\(Q\\big(s,a)\\longrightarrow q\\_{\\*}(s,a)\\\\), under the following conditions:\n - GLIE sequence of policies \\\\(\\pi\\_{t}(a \\bigm\\vert s)\\\\) \n - Robbins-Monro sequence of step-sizes \\\\(\\alpha\\_{t}\\\\)\n$$ \\sum\\_{t = 1}^{\\infty} \\alpha\\_{t} = \\infty $$\n$$ \\sum\\_{t = 1}^{\\infty} \\alpha\\_{t}^{2} \\lt \\infty $$"],"metadata":{}},{"cell_type":"markdown","source":["### Questions ###\nWhat graph do you expect to see when you use SARSA to train the agent? Can you plot Episodes vs Time steps for a typical RL problem?"],"metadata":{}},{"cell_type":"markdown","source":["### n-Step Sarsa ###\n\n- Similar to TD(\\\\(\\lambda\\\\))\n- We can define different q's\n$$ q\\_{t}^1 = R\\_{t+1} + \\gamma Q(S\\_{t+1}) $$\n$$ q\\_{t}^2 = R\\_{t+1} + \\gamma R\\_{t+2} + \\gamma^2 Q(S\\_{t+2}) $$\n.\n.\n.\n$$ q\\_{t}^\\infty = R\\_{t+1} + \\gamma R\\_{t+2} + ...+ \\gamma^{T-1} R\\_{T} $$\n$$ q\\_{t}^n = R\\_{t+1} + \\gamma R\\_{t+2} + ... + \\gamma^{n-1}R\\_{t+n} + \\gamma^{n} Q(S\\_{t+n}) $$\n$$ Q\\big(S, A\\big)\\longleftarrow Q\\big(S, A \\big) + \\alpha \\big(q\\_{t}^n -Q\\big(S, A\\big)\\big) $$"],"metadata":{}},{"cell_type":"markdown","source":["### Sarsa(\\\\(\\lambda\\\\)) Forward and Backward View\n- **Forward view:**\n$$ Q\\big(S, A\\big)\\longleftarrow Q\\big(S, A \\big) + \\alpha \\big(q\\_{t}^\\lambda -Q\\big(S, A\\big)\\big) $$\nwhere \n$$ q\\_{t}^\\lambda = (1-\\lambda) \\sum\\_{n = 1}^{\\infty}\\lambda^{n-1}q\\_{t}^n $$\n\n- **Backward view:**\n - Eligibility traces:\n - \\\\(E\\_{0}(s, a) = 0\\\\)\n - \\\\(E\\_{t}(s, a) = \\gamma\\lambda E\\_{t-1}(s, a) + 1(S\\_{t} = s, A\\_{t} = a)\\\\), where \\\\(1\\\\) is a indicator function\n- Keep an eligibility trace for every state s\n- Update Q(s, a) for every state s and action a\n$$\\delta\\_{t} = R\\_{t+1} + \\gamma Q(S\\_{t+1}, A\\_{t+1}) - Q(S\\_{t}, A\\_{t}) $$\n$$ Q(s, a) \\longleftarrow Q(s, a) + \\alpha \\delta\\_{t}E\\_{t}(s, a) $$"],"metadata":{}},{"cell_type":"markdown","source":["### Off-Policy Learning ###\n<br>\n\nYou might want to evaluate a **target** policy \\\\(\\pi(a \\bigm\\vert s)\\\\) to compute \\\\(v\\_{\\pi}(s)\\\\) or \\\\(q\\_{\\pi}(s)\\\\) while following a different **behavior** policy \\\\(\\mu(a \\bigm\\vert s)\\\\)"],"metadata":{}},{"cell_type":"markdown","source":["### Questions ###\n0. Why one would do that?\n0. How do you do that?"],"metadata":{}},{"cell_type":"markdown","source":["### Importance Sampling ###\n- Simple idea. Just re-order a computation to end up with a new probability distribution.\n$$ E\\_{X \\,\\, \\tilde{} \\,\\, P}  \\big[f(X) \\big] = \\sum P(X)f(X) = \\sum Q(X) \\frac{P(X)}{Q(X)} f(X) = E\\_{X \\,\\, \\tilde{} \\,\\, Q}\\bigg[f(X)\\frac{P(X)}{Q(X)}\\bigg] $$\n- Here how you apply this in practice:\n - Use returns generated from behavior policy \\\\(\\mu\\\\) to evaluate \\\\(\\pi\\\\)\n - If policies are similar (meaning the distribution is similar) then put height weight on the return. Otherwise, put less weight on the return\n - $$ G\\_{t}^{\\frac {\\pi}{\\mu}} = \\frac{\\pi\\big(A\\_{t}\\bigm\\vert S\\_{t}\\big)}{\\mu \\big(A\\_{t}\\bigm\\vert S\\_{t}\\big)} \\frac{\\pi\\big(A\\_{t+1}\\bigm\\vert S\\_{t+1}\\big)}{\\mu \\big(A\\_{t+1}\\bigm\\vert S\\_{t+1}\\big)} ... \\frac{\\pi\\big(A\\_{T}\\bigm\\vert S\\_{T}\\big)}{\\mu \\big(A\\_{T}\\bigm\\vert S\\_{T}\\big)} G\\_{t} $$\n - $$ V\\big(S\\_{t} \\big)\\longleftarrow V\\big(S\\_{t} \\big) + \\alpha \\big(G\\_{t}^{\\frac {\\pi}{\\mu}} -V\\big(S\\_{t}\\big)\\big) $$"],"metadata":{}},{"cell_type":"markdown","source":["### Question ###\n- What can go wrong with above formula?\n- Solutions?"],"metadata":{}},{"cell_type":"markdown","source":["- $$ V\\big(S\\_{t} \\big)\\longleftarrow V\\big(S\\_{t} \\big) + \\alpha \\bigg(\\frac{\\pi\\big(A\\_{t}\\bigm\\vert S\\_{t}\\big)}{\\mu \\big(A\\_{t}\\bigm\\vert S\\_{t}\\big)} (R\\_{t+1} + \\gamma V(S\\_{t+1})) -V\\big(S\\_{t}\\big)\\bigg) $$\n - Much lower variance. We do not multiply multiple ratios\n - Policies need to only be similar over a single step"],"metadata":{}},{"cell_type":"markdown","source":["### Q-Learning ###\n- It does not require importance sampling\n- It is off-policy learning\n- Next action is chosen based on behavior policy\n- Consider alternative actions\n- Update Q\n\n$$ Q\\big(S\\_{t}, A\\_{t} \\big)\\longleftarrow Q\\big(S\\_{t}, A\\_{t} \\big) + \\alpha  \\big(R\\_{t+1} + \\gamma Q(S\\_{t+1}, A') -Q\\big(S\\_{t}, A\\_{t}\\big)\\big) $$\n- Special case of above formula is when your policy is greedy policy\n - \\\\(\\pi\\big(S\\_{t+1}\\big) = argmax\\_{a'} Q\\big(S\\_{t+1}, a'\\big)\\\\)\n - The behavior policy is \\\\(\\epsilon\\\\)-greedy with respect to Q(s,a)\n - i.e \\\\(R\\_{t+1} + \\gamma Q(S\\_{t+1}, A') = R\\_{t+1} + max\\_{a'} \\gamma Q(S\\_{t+1}, a') \\\\)\n- Q-learning that you have heard about:\n$$ Q\\big(S\\_{t}, A\\_{t} \\big)\\longleftarrow Q\\big(S\\_{t}, A\\_{t} \\big) + \\alpha (R\\_{t+1} + max\\_{a'} \\gamma Q(S\\_{t+1}, a') -Q\\big(S\\_{t}, A\\_{t}\\big)\\big) $$"],"metadata":{}},{"cell_type":"markdown","source":["### Summary ###\n## ![comparision](https://chunpai.github.io/assets/img/DP_and_TD.png)"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"RL 05 - Model-free Control","notebookId":2929930686998431},"nbformat":4,"nbformat_minor":0}
