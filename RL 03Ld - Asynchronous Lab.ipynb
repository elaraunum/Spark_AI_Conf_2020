{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# Dynamic Programming: Asynchronous Lab\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lab you learn:<br>\n - Asynchronous update\n  \n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) References\n* Sutton Chapter 3"],"metadata":{}},{"cell_type":"markdown","source":["### Policy Iteration Asynchronously###\n<br>\nIn this lab we are going to find an optimal policy by implementing **asynchronous policy iteration** algorithm. Refer to the demo notebook to find proper formula for this lab.\n\n![gridenv](https://miro.medium.com/max/1000/1*iX-Fu5YzUZ8CNEZ86BvfKA.png)"],"metadata":{}},{"cell_type":"markdown","source":["Recreate the environment for the Dynamic Programming from the first lab."],"metadata":{}},{"cell_type":"code","source":["%run \"./helper/GridWorldAdvancedEnvironment\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["environment = GridWorldAdvancedEnvironment()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# ANSWER \ndef evaluate_policy_in_place(environment, policy, values, gamma=0.9): \n    \"\"\"Evaluate a policy given the full information of the environment. This is not the full RL.\"\"\"\n    \n    # Action's probability at any given state\n    action_probability = 1\n    \n    # Probability of going to state s' from s if you take an action\n    probability = 1\n    \n    for state in range(environment.ns):\n      # Make sure we cover all states\n      environment.set_state(state)\n    \n      # Take an action\n      next_state, reward, is_done, _= environment.step(policy[state])\n      # Use the formula to back up the values\n      values[state] = action_probability * (reward + gamma * probability * values[next_state])\n           \n\n    return values"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["import copy \n\ndef improve_policy(environment, policy_evaluate_function = evaluate_policy_in_place, iterations=20000,  gamma=0.9):\n  \"\"\"Iteratively select best action, update the state-value and repeat until there is no improvements.\"\"\"\n\n   # Define a deterministic policy. For example, assume no matter what state you are at, you can only go DOWN \n  policy = np.ones(environment.ns)\n  \n  # Initialize the values\n  values = np.zeros(environment.ns)\n  count = 0\n  \n  \n  while count < iterations:\n     \n    # Evaluate the policy first\n    values = policy_evaluate_function(environment, policy, values)\n    \n    # We need a copy\n    values_copy = copy.deepcopy(values)\n      \n    for state in range(environment.ns):\n      # Reset the environment\n      environment.set_state(state)\n      for action in range(4):\n        # Take an action\n        next_state, reward, is_done, _= environment.step(action)\n        # Act greedily. Take an action toward the higher values.\n        if values[next_state] >= values_copy[state]:\n          policy[state] = action\n          values_copy[state] = values[next_state]\n        environment.set_state(state)\n    \n    count += 1\n   \n  \n  return values, policy\n    "],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["optimal_values, optimal_policy = improve_policy(environment)\nprint(f\"Optimal Policy (0=UP, 1=DOWN, 2=RIGHT, 3=LEFT):\\n\\n {optimal_policy.reshape(5,5)}\\n\")\nprint(f\"State optimal values:\\n\\n {optimal_values.reshape(5,5)}\")"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# Test your code\nvalue_expected = np.array([[21.97, 24.41, 21.97, 19.41, 17.47] ,[19.77, 21.97, 19.77, 17.80, 16.02], [17.80, 19.77, 17.80, 16.02, 14.41 ], [16.02, 17.80, 16.02, 14.41,12.97],[14.41,  16.02, 14.41, 12.97, 11.67]])\nnp.testing.assert_array_almost_equal(optimal_values.reshape(5,5), value_expected, err_msg = \"The values are incorrect\", decimal=2)\n"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"RL 03Ld - Asynchronous Lab","notebookId":2929930686998345},"nbformat":4,"nbformat_minor":0}
