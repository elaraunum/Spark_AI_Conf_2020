{"cells":[{"cell_type":"code","source":["\nimport random\nimport numpy as np\nfrom statistics import mean\n\ndef monte_carlo(number_episodes):\n  '''\n  This function generates multiple episodes for the gridworld problem. \n  input:\n  number_episodes: int total number of episodes\n  output: list of episodes and list of rewards.  \n  '''\n  np.random.seed(1234)\n  # initial transition list. We will add to this list as we create new ones. In the end, we end up with list of the list.\n  visited_states = []\n  immediate_rewards = []\n  \n  # create samples of episodes\n  for i in range (number_episodes):\n    \n    # randomly pick the starting point\n    start_index = random.randint(1,14)\n    \n    # beginning of the episode. empty lists.\n    realized_states = []\n    realized_rewards = []\n    realized_states.append(start_index)\n    environment.set_state(start_index)\n    while True:\n      # randomly pick an action. Remember this is a random policy\n      action = random.randint(0,3)\n      # observe the state, reward, whether or not we have reached the terminal points\n      next_state, reward, is_done, _= environment.step(action)\n     \n    \n      \n      # keep the immediate reward\n      realized_rewards.append(reward)\n      \n      # leave if we are in the end\n      if is_done:\n        break\n      # record the next state\n      realized_states.append(next_state)\n      start_index = next_state\n    \n    # add the list to the final list. visited_states is the list of list. each list contains one of the episodes. \n    # immediate_rewards is list of list. each list contains the immediate rewards.\n    visited_states.append(realized_states)\n    immediate_rewards.append(realized_rewards)\n    \n  return visited_states, immediate_rewards\n"],"metadata":{},"outputs":[],"execution_count":1}],"metadata":{"name":"MonteCarloGridWorld","notebookId":2929930686998317},"nbformat":4,"nbformat_minor":0}
