{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# Every-visit MC Prediction - Gridworld Problem\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lab you learn:<br>\n - Policy Evaluation"],"metadata":{}},{"cell_type":"markdown","source":["### Problem Statement ###\n\nIn this lab we are going to evaluate a policy. Keep in mind that we do NOT know the dynamic of the environment nor are we given the MDP i.e. this is the full RL prediction problem. We created an environment for this gridworld problem earlier. We are going to use that environment to develop **MC every-visit** algorithm to evaluate a random policy.\n\n![Prediction](https://files.training.databricks.com/images/rl/prediction.png)"],"metadata":{}},{"cell_type":"code","source":["%run \"./helper/GridWorldEnvironment\""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%run \"./helper/MonteCarloGridWorld\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["environment = GridWorldEnvironment()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# ANSWER\ndef mc_every_visit(visited_states,immediate_rewards):\n  \"\"\"This function gets the list of episodes and list of immediate rewards associated with each episodes.\"\"\"\n  \n  # Initialize \n  cumulative_rewards = np.zeros(16)\n  cumulative_counts = np.ones(16)\n  i = 0\n\n  for episodes in visited_states:\n    j = 0\n    for state in episodes:\n      # Calculate the cumulative reward and count for each episode\n      cumulative_rewards[state] += np.sum(immediate_rewards[i][j:])\n      cumulative_counts[state] += 1 \n      j += 1\n    i += 1\n      \n  return cumulative_rewards, cumulative_counts"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["visited_states, immediate_rewards = monte_carlo(100000)\ncumulative_rewards, cumulative_counts = mc_every_visit(visited_states,immediate_rewards)\nvalue = cumulative_rewards/cumulative_counts"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# Test your code\nvalue_expected = [0.0, -14.0, -19.9, -21.0, -14.0, -18.0, -19.9, -19.0, -20.0, -19.0, -17.0, -13.0, -21.0, -19.0, -13.0, 0.0 ]  \nnp.testing.assert_array_almost_equal(value, value_expected, err_msg = \"The values are incorrect\", decimal = 0)\n"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"RL 04Le - Every-visit MC Prediction Lab - Gridworld Problem","notebookId":2929930686998280},"nbformat":4,"nbformat_minor":0}
