{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n# OpenAI gym Lab\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you learn:<br>\n - How to use OpenAI gym\n  \n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Library Requirements\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Additional libraries must be attached to your cluster for this lesson to work.\n\nWe will use the PyPI library **`gym==0.15.4`**.\n* This library is used to develop and compare reinforcement learning algorithms\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) References\n* [Gym documentation](https://gym.openai.com/docs/)\n* Sutton book - Chapter 2"],"metadata":{}},{"cell_type":"markdown","source":["### Exercise 1\n0. Familiarize yourself with OpenAI gym. Click [here](https://gym.openai.com/docs/)\n0. Familiarize yourself with Env class. And how you can extend that. Click [here](https://github.com/openai/gym/blob/master/gym/envs/toy_text/discrete.py)"],"metadata":{}},{"cell_type":"markdown","source":["### Exercise 2\nConsider the following environment:\n\n1. There is a 5 by 5 grid.\n2. There are 4 actions: UP, DOWN, LEFT and RIGHT. Each action results in a move. The reward for each action is 0 unless you try to leave the grid i.e. you are at the edges. If you are at the edge and you decide to leave the grid, the reward is -1. If you are at A and B, ANY action results in the move shown below and you receive +10 and +5 rewards, respectively.\n4. Create GridWorldAdvancedEnvironment class to represent the environment for this problem.\n<br>\n![gridenv](https://files.training.databricks.com/images/rl/actions_ab.png)"],"metadata":{}},{"cell_type":"code","source":["# ANSWER\n# Required libraries\nimport gym\nimport numpy as np\nfrom gym import spaces\nnp.random.seed(1234)\n\n\nclass GridWorldAdvancedEnvironment(gym.Env):\n  \"\"\"This class describes the simple version of Deal or No Deal environment\"\"\"\n  \n  def __init__(self):\n    \n    # Define the immediate reward, number of actions and number of states\n    \n    self.UP = 0\n    self.DOWN = 1\n    self.RIGHT = 2\n    self.LEFT = 3\n    \n    # Number of actions\n    self.na = 4\n    # Number of states\n    self.ns = 25\n    # Initial state\n    self.state = 0\n    \n\n  # Define reset() method. This method reset the environment.\n  def reset(self, state):\n    return self._reset(state)\n  \n  # Define step() method. This method describes how the environment responds. \n  def step(self, action):\n    return self._step(action)\n  \n  \n  \n  def _step(self, action):\n  \n    \n    # Going up\n    if action == self.UP:\n      \n      if self.state in range(5) and self.state != 1 and self.state != 3:\n        self.state = self.state\n        self.reward = -1\n      \n      elif self.state == 1:\n        self.reward = 10\n        self.state = 21\n      \n      elif self.state == 3:\n        self.reward = 5\n        self.state = 13\n      \n      else:\n        self.reward = 0\n        self.state -= 5\n        \n    # Going down    \n    if action == self.DOWN:\n      \n      if self.state in range(20, 25) and self.state != 1 and self.state != 3:\n        self.state = self.state\n        self.reward = -1\n        \n      elif self.state == 1:\n        self.reward = 10\n        self.state = 21\n      \n      elif self.state == 3:\n        self.reward = 5\n        self.state = 13\n      \n      else:\n        self.reward = 0\n        self.state += 5\n        \n        \n    # Going right   \n    if action == self.RIGHT:\n      \n      if self.state in range(4, 25, 5) and self.state != 1 and self.state != 3:\n        self.state = self.state\n        self.reward = -1\n        \n      elif self.state == 1:\n        self.reward = 10\n        self.state = 21\n      \n      elif self.state == 3:\n        self.reward = 5\n        self.state = 13\n      \n      else:\n        self.reward = 0\n        self.state += 1\n        \n        \n    # Going left    \n    if action == self.LEFT:\n      \n      \n      if self.state in range(0, 21, 5) and self.state != 1 and self.state != 3:\n        self.state = self.state\n        self.reward = -1\n        \n      elif self.state == 1:\n        self.reward = 10\n        self.state = 21\n      \n      elif self.state == 3:\n        self.reward = 5\n        self.state = 13\n      else:\n        self.reward = 0\n        self.state -= 1\n  \n    \n    self.is_done = False\n  \n    \n    return [self.state, self.reward, self.is_done, {}]\n  \n  # This method reset the episode \n  def _reset(self, state):\n    # Initial state\n    self.state = state "],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["### Exercise 3\nGridworld example is shown below for your reference.Now that you have created this class, complete the following steps:<br><br>\n\n0. Create an object of this class.\n0. Explore the properties of the object:\n - How many actions are there?\n - How many states are there?\n - What are the possible actions for state 0 and 5?\n0. For a random policy, what is the probability of moving up given you are at state 8? How about for state 10? \n0. What is the probability of moving up at \\\\(t\\_{10}\\\\) if you are at state 9 given you were at state 6 at \\\\(t_0\\\\)?\n\n![gridenv](https://files.training.databricks.com/images/rl/actions_ab.png)"],"metadata":{}},{"cell_type":"code","source":["# ANSWER\n# Answers to q1 and q2\nmy_obj = GridWorldAdvancedEnvironment()\nprint(f\" There are {my_obj.na} actions.\")\nprint(f\" There are {my_obj.ns} states.\")\nprint(f\" Possible actions for state 0 are UP, DOWN, LEFT, RIGHT\")\nprint(f\" Possible actions for state 5 are UP, DOWN, LEFT, RIGHT\")\n\n\n# Answers to q3 and q4\n# 3. 0.25, 0.25\n# 4. 0.25\n"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"RL 01L - OpenAI gym Lab","notebookId":2929930686998219},"nbformat":4,"nbformat_minor":0}
