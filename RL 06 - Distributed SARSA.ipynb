{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# Distributed SARSA Control - Gridworld Problem\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lab you learn:<br>\n - Finding Optimal Policy Using Distributed SARSA\n \nRefrences: [Horovod](https://horovod.readthedocs.io/en/latest/)"],"metadata":{}},{"cell_type":"markdown","source":["### Problem Statement ###\n\nIn this lab we are going to find an optimal policy in a **distributed** fashion by using **Horovod**. Keep in mind that we do NOT know the dynamic of the environment nor are we given the MDP i.e. this is the full RL prediction problem. We created an environment for this gridworld problem earlier. We are going to use that environment to develop SARSA algorithm to find optimal policy.\n\n![Prediction](https://files.training.databricks.com/images/rl/prediction.png)"],"metadata":{}},{"cell_type":"markdown","source":["## What is Horovod?\n  \n\"Horovod is a distributed deep learning training framework for TensorFlow, Keras, PyTorch, and Apache MXNet. It was developed at Uber. The goal of Horovod is to make distributed deep learning fast and easy to use. The primary motivation for Horovod is to make it easy to take a single-GPU training script and successfully scale it to train across many GPUs in parallel\". For more information, see [Horovod](https://github.com/uber/horovod)."],"metadata":{}},{"cell_type":"code","source":["%run \"./Labs/helper/GridWorldEnvironment\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["environment = GridWorldEnvironment()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["import numpy as np\nimport random\nnp.random.seed(1234)\n\n\ndef pick_action(Q, na, epsilon=0.1):\n  \"\"\"This function picks the greedy action according to e-greedy algorithm.\"\"\"\n  \n  actions = set([0,1,2,3])\n  \n  # Pick the greedy action with probability greater than epsilon/na \n  if np.random.rand() >= epsilon/na :\n    return np.argmax(Q)\n  # Pick a non-greedy action with a probability of epsilon/na \n  else:\n    return random.sample(actions - set([np.argmax(Q)]), 1)[0]\n      \n  \ndef sarsa(Q, ns, na, gamma=1.0, epsilon=0.1, alpha=0.1, number_of_iterations=100000):\n  \"\"\"This function implements SARSA algorithm.\"\"\"\n \n  # Number of time steps\n  time_step = 1000\n  \n  # Create samples of episodes\n  for i in range(number_of_iterations):\n    if i%10000 == 0:\n      print(f\"This is iteration {i+1}\")\n    # Initial start point\n    start_state_index = random.randint(1,14)\n    action_index = pick_action(Q[start_state_index][:], na)\n    environment.set_state(start_state_index)\n    \n    for j in range(time_step):\n      # Take an action and observe next state, reward and whether or not we are at the terminal points\n      next_state, reward, is_done, _ = environment.step(action_index)\n      \n      next_action_index = pick_action(Q[next_state][:], na)\n      #Update the Q\n      Q[start_state_index][action_index] = Q[start_state_index][action_index] + alpha * (reward + gamma * Q[next_state][next_action_index] - Q[start_state_index][action_index])\n      \n      # Leave the loop if at terminal points\n      if is_done:\n        break\n      start_state_index = next_state\n      action_index = next_action_index\n         \n  return Q"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["import numpy as np\nimport horovod.tensorflow.keras as hvd\n\ndef run(Q, na, ns):\n  \"\"\"This function runs SARSA algorithm in a distributed fashion using Horovod.\"\"\"\n  \n  hvd.init()\n  Q = np.zeros([ns, na])\n  new_q = sarsa(Q, ns, na)\n  return new_q\n  "],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["import horovod.spark\n\ndef main(iteration):\n  \"\"\"This function invokes Horovod for multiple iterations.\"\"\"\n  ns = 16\n  na = 4\n  Q = np.zeros([ns, na])\n  for i in range(iteration):\n    Q = np.array(horovod.spark.run(run, (Q, na, ns)))\n  return Q[0]"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["Q = main(10)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# Test your code\nvalue_expected = [[ 0.0,  0.0,  0.0,  0.0],\n       [-2.0, -3.0, -3.0, -1.0],\n       [-3.0, -4.0, -4.0, -2.0],\n       [-4.1, -4.0, -3.3, -3.1],\n       [-1.0, -3.0, -3.0, -2.0],\n       [-2.0, -4.0, -4.0, -2.3],\n       [-3.3, -3.0, -3.0, -3.0],\n       [-4.0, -3.0, -2.0, -4.0],\n       [-2.0, -4.0, -4.0, -3.0],\n       [-3.0, -3.0, -3.0, -3.0],\n       [-4.0, -2.0, -2.0, -4.0],\n       [-3.0, -2.0, -1.0, -3.0],\n       [-3.0, -3.0, -4.0, -4.0],\n       [-4.0, -2.0, -3.0, -4.0],\n       [-3.1, -1.0, -2.0, -3.0],\n       [ 0.0,  0.0,  0.0,  0.0]]  \nnp.testing.assert_array_almost_equal(Q, value_expected, err_msg = \"The values are incorrect\", decimal = 0)\n"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"RL 06 - Distributed SARSA","notebookId":2929930686998084},"nbformat":4,"nbformat_minor":0}
