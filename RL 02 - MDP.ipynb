{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 1200px\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["# MDPs and Bellman Equations\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you learn:<br>\n - Agent-Environment interactions\n - Markov processes\n  * Probability transitions\n  * Episodes\n - Markov Reward Processes\n  * Reward \n  * Return\n  * Discount factors\n  * Value functions\n  * Bellman equation\n - Markov Decision Processes\n  * Policy \n  * State-value function\n  * Action-value function\n  * Bellman equations revisited\n  * Optimal value function\n  * Optimal policy \n<br>\n\n### Out of scope\n- [POMDPs](https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process)\n- Continuous MDPs\n- Infinite MDPs\n- Undiscounted MDPs\n  \n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) References\n* [David Silver lecture](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ)\n* Sutton book - Chapter 2 & 3"],"metadata":{}},{"cell_type":"markdown","source":["### Markovian Process\n\n0. Markov Decision Process formally describes agent<>environment setting\n0. Reminder: Environment is fully observable. What does that mean?\n0. All RL problems can be formulated as such\n0. Markov Property (reminder):\n$$ P[ S\\_{t+1} \\bigm\\vert S\\_{t} ] = P[ S\\_{t+1} \\bigm\\vert S\\_{1},S\\_{2},...,S\\_{t} ] $$\n * What is the interpretation?"],"metadata":{}},{"cell_type":"markdown","source":["### Transition Matrix\n\n\\\\(\\rho\\_{ss'} = P[ S\\_{t+1} = s' \\bigm\\vert S\\_{t} = s]\\\\).\n\nThis formula represents the probability transition between any two states. The probability of going to s' when you are at state s. We can summarize all of the transitions in a matrix:\n\n\\\\(\\rho = \\begin{pmatrix} \n\\rho\\_{11} & \\rho\\_{12} & ... & \\rho\\_{1n}\n\\\\\\ .\n\\\\\\ .\n\\\\\\ .\n\\\\\\ \\rho\\_{n1} & \\rho\\_{n2}& ... & \\rho\\_{nn} \\end{pmatrix} \\\\)\n\n**Question:**\n<br/>\nWhat can you say about sum of each row and columns?\n<br/><br/><br/>\n**Markov Process** also known as Markov Chain:\n\n0. It is tuple \\\\(\\langle S,\\rho \\rangle\\\\)\n0. S is finite\n0. \\\\(\\rho\\\\) is state transition probability matrix"],"metadata":{}},{"cell_type":"markdown","source":["### Exercise 1 ###\n<br/><br/>\n![MDPs](https://files.training.databricks.com/images/rl/mdps.png)\n<br>\nFor above graph:\n0. Write \\\\(\\rho\\\\), the state transition probability matrix.\n0. What is the probability of ending in F given you are at state S?\n0. What is the probability of ending in F after two tries given you are at state S? What are the assumptions behind your answer?"],"metadata":{}},{"cell_type":"markdown","source":["### Markov Reward Process ###\nMarkov Reward Process is a Markov chain \\\\(\\langle S, \\rho, R, \\gamma \\rangle\\\\) such that:\n0. \\\\(P[ S\\_{t+1}, R\\_{t+1} \\bigm\\vert S\\_{t} ] = P[ S\\_{t+1}, R\\_{t+1} \\bigm\\vert S\\_{1},S\\_{2},...,S\\_{t}]\\\\)\n0. \\\\(R\\\\) is a **reward function**. That is \\\\(R\\_{s} = E[R\\_{t+1} \\bigm\\vert S_{t} = s]\\\\)\n0. \\\\(\\gamma\\\\) is a discount factor. \\\\(\\gamma \\in [0,1]\\\\).\n\n**Return** is the total sum of discounted reward:\n\\\\(G\\_{t} = R\\_{t+1}+ \\gamma R\\_{t+2} + \\gamma^2 R\\_{t+3}+ ...\\\\)\n<br/><br/>\n**Questions**\n0. Why \\\\(\\gamma \\in [0,1]\\\\)? What happens when you have cyclic cases?\n0. What does \\\\(\\gamma = 0 \\\\) imply? In what scenario do you use that?\n0. What does \\\\(\\gamma = 1 \\\\) imply? In what scenario do you use that?"],"metadata":{}},{"cell_type":"markdown","source":["### Value Function ###\n\n0. Long term value of state \\\\(s\\\\). i.e what we expect to get as return given we are at state \\\\(s\\\\)\n$$ v(s) = E[G\\_{t} \\bigm\\vert S\\_{t} = s] $$"],"metadata":{}},{"cell_type":"markdown","source":["### Exercise 2 ###\n\nRefer to exercise 1 graph. Assume the return of state **\\\\(S, R, F\\\\) are -1, -3, 1**, respectively.\n<br/><br/><br/>\nFor each of the following episodes calculate \\\\(G\\_{1}\\\\). Assume we start at \\\\(S\\\\) and \\\\(\\gamma = 0.5 \\\\). You may want to write a code to do the computation. \n0. \\\\(S, S, S, S, ...\\\\)\n0. \\\\(S, R, S, R, S, R, S, R, S, R, ...\\\\)\n0. \\\\(S, R, F, S, R, F, S, R, F, ...\\\\)"],"metadata":{}},{"cell_type":"markdown","source":["### Bellman Equations for Markov Reward Processes ###\n\n\\\\( \\begin{aligned} \\\\\\ v(s) &= E[G\\_{t} \\bigm\\vert S\\_{t} = s] \\\\\\  &= E[R\\_{t+1} + \\gamma R\\_{t+2} + \\gamma^2 R\\_{t+2}+ ... \\bigm\\vert S\\_{t} = s]  \\\\\\   &= E[R\\_{t+1} + \\gamma (R\\_{t+2} + \\gamma R\\_{t+2}+ ...) \\bigm\\vert S\\_{t} = s] \\\\\\\n&= E[R\\_{t+1} + \\gamma G\\_{t+1} \\bigm\\vert S\\_{t} = s]\\\\\\\n&= E[R\\_{t+1} + \\gamma v(s\\_{t+1}) \\bigm\\vert S\\_{t} = s]\n\\end{aligned}\\\\)"],"metadata":{}},{"cell_type":"markdown","source":["### Questions ###\n\n0. What is computational complexity for solving above system of linear equations?\n0. Find eigenvalues and eigenvectors of transition probability matrix in previous exercise. What do they signify?\n0. How would you solve approach large MRPs?\n  - Dynamic Programming (more to come later)\n  - MC evaluation (more to come)\n  - Temporal-difference Learning (if time permits)"],"metadata":{}},{"cell_type":"markdown","source":["### Markov Decision Processes (MDPs) & Policies ###\n\n**MDP** is a tuple \\\\(\\langle S, A, \\rho, R, \\gamma \\rangle\\\\) such that:\n0. \\\\(P[ S\\_{t+1}, R\\_{t+1} \\bigm\\vert S\\_{t}, A\\_{t} ] = P[ S\\_{t+1}, R\\_{t+1} \\bigm\\vert S\\_{1},A\\_{1},S\\_{2},A\\_{2}...,S\\_{t},A\\_{t} ]\\\\)\n0. A is finite set of actions\n0. \\\\(\\rho^a\\_{ss'} = P[ S\\_{t+1} = s' \\bigm\\vert S\\_{t} = s, A\\_{t} = a]\\\\)\n0. R is a reward function i.e. \\\\(R^a\\_{s} = E[R\\_{t+1} \\bigm\\vert S\\_{t} = s, A\\_{t} = a]\\\\)\n<br/><br/>\n\nA **Policy** is a distribution over actions given the states. i.e how actions are distributed for a given state. Formally it can written as:\n$$ \\pi(a \\bigm\\vert s) = P[A\\_{t} = a \\bigm\\vert S_{t} = s] $$\n\n0. If you know the policy, you know how an agent will behave\n0. Remember definition of MDPs: MDP policies only depend on current states. You can throw away all history!\n0. What is the other name for 2?\n\n### Questions ###\n0. If we have a MDP, is the sequence of states \\\\(S\\_{1}, S\\_{2}, S\\_{3}, S\\_{4}... \\\\) MP? If so, write down probability transition formula from \\\\(s\\\\) to \\\\(s'\\\\) under policy \\\\(\\pi\\\\) i.e. \\\\(\\rho^\\pi\\_{s,s'}\\\\) If not, why not?\n0. Is the sequence of \\\\(S\\_{1},R\\_{2}, S\\_{2}, R\\_{3}, S\\_{4}... \\\\) MRP? If so, write down reward function formula \\\\(R^\\pi\\_{s}\\\\). If not, why not?\n0. How does **value function** change for MDP? Can you write its formula?"],"metadata":{}},{"cell_type":"markdown","source":["### State-value and action-value functions ###\n$$ v\\_{\\pi}(s) = E\\_{\\pi}[G\\_{t} \\bigm\\vert S\\_{t} = s] $$\n$$ q\\_{\\pi}(s, a) = E\\_{\\pi}[G\\_{t} \\bigm\\vert S\\_{t} = s, A\\_{t} = a] $$"],"metadata":{}},{"cell_type":"markdown","source":["### Bellman Expectation Equation ###\n\n\\\\( \\begin{aligned} \\\\\\ q\\_{\\pi}(s,a) &= E\\_{\\pi}[G\\_{t} \\bigm\\vert S\\_{t} = s, A\\_{t} = a] \\\\\\  &= E\\_{\\pi}[R\\_{t+1} + \\gamma R\\_{t+2} + \\gamma^2 R\\_{t+3}+ ... \\bigm\\vert S\\_{t} = s, A\\_{t} = a]  \\\\\\   &= E\\_{\\pi}[R\\_{t+1} + \\gamma (R\\_{t+2} + \\gamma R\\_{t+3}+ ...) \\bigm\\vert S\\_{t} = s, A\\_{t} = a] \\\\\\\n&= E\\_{\\pi}[R\\_{t+1} + \\gamma G\\_{t+1} \\bigm\\vert S\\_{t} = s, A\\_{t} = a]\\\\\\\n&= E\\_{\\pi}[R\\_{t+1} + \\gamma q\\_{\\pi}(s\\_{t+1}, A\\_{t+1}) \\bigm\\vert S\\_{t} = s, A\\_{t} = a]\\\\\\\n&= R^a\\_{s}+\\gamma \\sum\\_{s'\\in S} \\rho\\_{ss'}^a v\\_{\\pi}(s')\\\\\\\n&= R^a\\_{s}+\\gamma \\sum\\_{s'\\in S} \\rho\\_{ss'}^a \\sum\\_{a'\\in A} \\pi(a'\\bigm\\vert s') q\\_{\\pi}(s',a')\n\\end{aligned}\\\\)\n\n\n\\\\( \\begin{aligned} \\\\\\ v\\_{\\pi}(s) &= E\\_{\\pi}[G\\_{t} \\bigm\\vert S\\_{t} = s] \\\\\\  &= E\\_{\\pi}[R\\_{t+1} + \\gamma R\\_{t+2} + \\gamma^2 R\\_{t+3}+ ... \\bigm\\vert S\\_{t} = s]  \\\\\\   &= E\\_{\\pi}[R\\_{t+1} + \\gamma (R\\_{t+2} + \\gamma R\\_{t+3}+ ...) \\bigm\\vert S\\_{t} = s] \\\\\\\n&= E\\_{\\pi}[R\\_{t+1} + \\gamma G\\_{t+1} \\bigm\\vert S\\_{t} = s]\\\\\\\n&= E\\_{\\pi}[R\\_{t+1} + \\gamma v\\_{\\pi}(s\\_{t+1}) \\bigm\\vert S\\_{t} = s]\\\\\\\n&= \\sum\\_{a\\in A} \\pi(a\\bigm\\vert s) q\\_{\\pi}(s,a)\\\\\\\n&= \\sum\\_{a\\in A} \\pi(a\\bigm\\vert s) (R^a\\_{s}+\\gamma \\sum\\_{s'\\in S} \\rho\\_{ss'}^a v\\_{\\pi}(s'))\n\\end{aligned}\\\\)\n\n\n###Questions###\n0. Can you flatten MDP to MRP? If so how?"],"metadata":{}},{"cell_type":"markdown","source":["### Optimal Value functions ###\n$$ v\\_{*}(s) = max\\_{\\pi} v\\_{\\pi}(s) $$ \n\n$$ q\\_{*}(s, a) = max\\_{\\pi} q\\_{\\pi}(s,a) $$\n\n**Optimal state-value function** is the maximum value of the function over all policies. Similarly, **optimal action-value function** is the maximum action-value function over all policies.\n\n### Optimal Policy ###\n\\\\(\\pi \\geq \\pi' \\\\) if \\\\(v\\_{\\pi} \\geq v\\_{\\pi'}, \\forall s \\\\).\n\n###Theorem###\nFor any Markov Decision Process.\n0. There exists an optimal policy \\\\(\\pi\\_{*}\\\\) that is better than or equal to all other policies, \\\\(\\pi \\geq \\pi' \\forall \\pi \\\\)\n0. All optimal policies achieve the optimal value function, \\\\(v\\_{\\pi\\_{\\*}} = v\\_{*}(s) \\\\).\n0. All optimal policies achieve the optimal action-value function, \\\\(q\\_{\\pi\\_{\\*}}(s,a) = q\\_{*}(s,a) \\\\)."],"metadata":{}},{"cell_type":"markdown","source":["### How can you find an optimal policy (more on this later) ? ### \n\n0. It can be found by maximizing over \\\\(q\\_{*}(s,a)\\\\). i.e pick an action that maximizes q at each state\n0. There is always a deterministic optimal policy for any MDP\n0. If we know \\\\(q\\_{*}(s,a)\\\\), we have the optimal policy. i.e. the agent knows what to do at each state"],"metadata":{}},{"cell_type":"markdown","source":["### Putting everything together, Bellman Optimality equations ###\n\n$$ \\begin{aligned} \\\\\\ v\\_{\\*}(s) &= max\\_{a}q\\_{\\*}(s,a)\\\\\\ &= max\\_{a} R\\_{s}^a + \\gamma \\sum\\_{s'\\in S} \\rho\\_{ss'}^a v\\_{\\*}(s') \\end{aligned} $$\n\n$$ \\begin{aligned} \\\\\\ q\\_{\\*}(s,a) &= R^a\\_{s}+\\gamma \\sum\\_{s'\\in S} \\rho\\_{ss'}^a v\\_{\\*}(s') \\\\\\ &= R^a\\_{s}+\\gamma \\sum\\_{s'\\in S} \\rho\\_{ss'}^a max\\_{a'}q\\_{\\*}(s',a')  \\end{aligned} $$"],"metadata":{}},{"cell_type":"markdown","source":["### Solving Bellman Optimality Equations (more on this later) ###\n\n0. Non-linear. Cannot solve it with Linear Algebra\n0. No closed-form solution. Yes, in special cases we have closed form solution\n0. Approaches:\n - Value iteration\n - Policy iteration\n - Q learning\n - Sarsa"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{}}],"metadata":{"name":"RL 02 - MDP","notebookId":2929930686998065},"nbformat":4,"nbformat_minor":0}
